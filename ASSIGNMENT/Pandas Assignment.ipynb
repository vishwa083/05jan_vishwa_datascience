{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fd2f0b",
   "metadata": {},
   "source": [
    "# Ex2 -  Getting and knowing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fd2dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id  quantity                              item_name  \\\n",
      "0         1         1           Chips and Fresh Tomato Salsa   \n",
      "1         1         1                                   Izze   \n",
      "2         1         1                       Nantucket Nectar   \n",
      "3         1         1  Chips and Tomatillo-Green Chili Salsa   \n",
      "4         2         2                           Chicken Bowl   \n",
      "5         3         1                           Chicken Bowl   \n",
      "6         3         1                          Side of Chips   \n",
      "7         4         1                          Steak Burrito   \n",
      "8         4         1                       Steak Soft Tacos   \n",
      "9         5         1                          Steak Burrito   \n",
      "\n",
      "                                  choice_description item_price  \n",
      "0                                                NaN     $2.39   \n",
      "1                                       [Clementine]     $3.39   \n",
      "2                                            [Apple]     $3.39   \n",
      "3                                                NaN     $2.39   \n",
      "4  [Tomatillo-Red Chili Salsa (Hot), [Black Beans...    $16.98   \n",
      "5  [Fresh Tomato Salsa (Mild), [Rice, Cheese, Sou...    $10.98   \n",
      "6                                                NaN     $1.69   \n",
      "7  [Tomatillo Red Chili Salsa, [Fajita Vegetables...    $11.75   \n",
      "8  [Tomatillo Green Chili Salsa, [Pinto Beans, Ch...     $9.25   \n",
      "9  [Fresh Tomato Salsa, [Rice, Black Beans, Pinto...     $9.25   \n",
      "Number of observations in the following dataset is :4622\n",
      "Number of columns in the following dataset is :5\n",
      "Name of the columns in the dataset :Index(['order_id', 'quantity', 'item_name', 'choice_description',\n",
      "       'item_price'],\n",
      "      dtype='object')\n",
      "Indexing of the dataset :RangeIndex(start=0, stop=4622, step=1)\n",
      "The most ordered item is :Chicken Bowl\n",
      "The number of Chicken Bowl ordered : 761\n",
      "The most ordered item in choice description column is :[Diet Coke]\n",
      "Total number of items ordered :4972\n",
      "\n",
      "Step 13.a. Item price type:\n",
      "object\n",
      "\n",
      "Step 13.c. Item price type after conversion:\n",
      "float64\n",
      "\n",
      "Step 14. Revenue for the period: $39237.02\n",
      "\n",
      "Step 15. Total orders made: 1834\n",
      "\n",
      "Step 16. Average revenue amount per order: $21.39\n",
      "\n",
      "Step 17. Number of different items sold: 50\n"
     ]
    }
   ],
   "source": [
    "#Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "#Step 2. Import the dataset from this address.f\"\"\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\"\n",
    "\n",
    "#Step 3. Assign it to a variable called chipo.\n",
    "chipo=pd.read_csv(url, sep='\\t')\n",
    "\n",
    "#Step 4. See the first 10 entries\n",
    "print(chipo.head(10))\n",
    "\n",
    "#Step 5. What is the number of observations in the dataset?\n",
    "num_obs=len(chipo)\n",
    "print(f\"Number of observations in the following dataset is :{num_obs}\")\n",
    "\n",
    "#Step 6. What is the number of columns in the dataset?\n",
    "num_col=len(chipo.columns)\n",
    "print(f\"Number of columns in the following dataset is :{num_col}\")\n",
    "\n",
    "#Step 7. Print the name of all the columns.\n",
    "print(f\"Name of the columns in the dataset :{chipo.columns}\")\n",
    "\n",
    "#Step 8. How is the dataset indexed?\n",
    "print(f\"Indexing of the dataset :{chipo.index}\")\n",
    "\n",
    "#Step 9. Which was the most-ordered item?\n",
    "most_ord_itm=chipo['item_name'].value_counts().idxmax()\n",
    "print(f\"The most ordered item is :{most_ord_itm}\")\n",
    "\n",
    "#Step 10. For the most-ordered item, how many items were ordered?\n",
    "most_ord_itm_quan=chipo[chipo['item_name']==most_ord_itm]['quantity'].sum()\n",
    "print(f\"The number of {most_ord_itm} ordered : {most_ord_itm_quan}\")\n",
    "\n",
    "#Step 11. What was the most ordered item in the choice_description column?\n",
    "most_ord_itm_des=chipo['choice_description'].value_counts().idxmax()\n",
    "print(f\"The most ordered item in choice description column is :{most_ord_itm_des}\")\n",
    "\n",
    "#Step 12. How many items were orderd in total?\n",
    "total_items_ordered=chipo['quantity'].sum()\n",
    "print(f\"Total number of items ordered :{total_items_ordered}\")\n",
    "\n",
    "#Step 13. Turn the item price into a float\n",
    "#Step 13.a. Check the item price type\n",
    "print(\"\\nStep 13.a. Item price type:\")\n",
    "print(chipo['item_price'].dtype)\n",
    "#Step 13.b. Create a lambda function and change the type of item price\n",
    "chipo['item_price'] = chipo['item_price'].apply(lambda x: float(x[1:]))\n",
    "#Step 13.c. Check the item price type\n",
    "print(\"\\nStep 13.c. Item price type after conversion:\")\n",
    "print(chipo['item_price'].dtype)\n",
    "\n",
    "#Step 14. How much was the revenue for the period in the dataset?\n",
    "revenue = (chipo['quantity'] * chipo['item_price']).sum()\n",
    "print(f\"\\nStep 14. Revenue for the period: ${revenue:.2f}\")\n",
    "\n",
    "# Step 15. How many orders were made in the period?\n",
    "total_orders = chipo['order_id'].nunique()\n",
    "print(f\"\\nStep 15. Total orders made: {total_orders}\")\n",
    "\n",
    "# Step 16. What is the average revenue amount per order?\n",
    "average_revenue_per_order = revenue / total_orders\n",
    "print(f\"\\nStep 16. Average revenue amount per order: ${average_revenue_per_order:.2f}\")\n",
    "\n",
    "# Step 17. How many different items are sold?\n",
    "unique_items_sold = chipo['item_name'].nunique()\n",
    "print(f\"\\nStep 17. Number of different items sold: {unique_items_sold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e94827",
   "metadata": {},
   "source": [
    "# Ex3 - Getting and Knowing your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5d2d1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 25 entries:\n",
      "         age gender     occupation zip_code\n",
      "user_id                                    \n",
      "1         24      M     technician    85711\n",
      "2         53      F          other    94043\n",
      "3         23      M         writer    32067\n",
      "4         24      M     technician    43537\n",
      "5         33      F          other    15213\n",
      "6         42      M      executive    98101\n",
      "7         57      M  administrator    91344\n",
      "8         36      M  administrator    05201\n",
      "9         29      M        student    01002\n",
      "10        53      M         lawyer    90703\n",
      "11        39      F          other    30329\n",
      "12        28      F          other    06405\n",
      "13        47      M       educator    29206\n",
      "14        45      M      scientist    55106\n",
      "15        49      F       educator    97301\n",
      "16        21      M  entertainment    10309\n",
      "17        30      M     programmer    06355\n",
      "18        35      F          other    37212\n",
      "19        40      M      librarian    02138\n",
      "20        42      F      homemaker    95660\n",
      "21        26      M         writer    30068\n",
      "22        25      M         writer    40206\n",
      "23        30      F         artist    48197\n",
      "24        21      F         artist    94533\n",
      "25        39      M       engineer    55107\n",
      "\n",
      "Last 10 entries:\n",
      "         age gender     occupation zip_code\n",
      "user_id                                    \n",
      "934       61      M       engineer    22902\n",
      "935       42      M         doctor    66221\n",
      "936       24      M          other    32789\n",
      "937       48      M       educator    98072\n",
      "938       38      F     technician    55038\n",
      "939       26      F        student    33319\n",
      "940       32      M  administrator    02215\n",
      "941       20      M        student    97229\n",
      "942       48      F      librarian    78209\n",
      "943       22      M        student    77841\n",
      "\n",
      "Number of observations: 943\n",
      "\n",
      "Number of columns: 4\n",
      "\n",
      "Column names:\n",
      "Index(['age', 'gender', 'occupation', 'zip_code'], dtype='object')\n",
      "\n",
      "Dataset indexing:\n",
      "Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n",
      "       ...\n",
      "       934, 935, 936, 937, 938, 939, 940, 941, 942, 943],\n",
      "      dtype='int64', name='user_id', length=943)\n",
      "\n",
      "Data types of each column:\n",
      "age            int64\n",
      "gender        object\n",
      "occupation    object\n",
      "zip_code      object\n",
      "dtype: object\n",
      "\n",
      "Occupation column:\n",
      "user_id\n",
      "1         technician\n",
      "2              other\n",
      "3             writer\n",
      "4         technician\n",
      "5              other\n",
      "           ...      \n",
      "939          student\n",
      "940    administrator\n",
      "941          student\n",
      "942        librarian\n",
      "943          student\n",
      "Name: occupation, Length: 943, dtype: object\n",
      "\n",
      "Number of different occupations: 21\n",
      "\n",
      "Most frequent occupation:\n",
      "student\n",
      "\n",
      "DataFrame summary:\n",
      "              age\n",
      "count  943.000000\n",
      "mean    34.051962\n",
      "std     12.192740\n",
      "min      7.000000\n",
      "25%     25.000000\n",
      "50%     31.000000\n",
      "75%     43.000000\n",
      "max     73.000000\n",
      "\n",
      "Summary of all columns:\n",
      "               age gender occupation zip_code\n",
      "count   943.000000    943        943      943\n",
      "unique         NaN      2         21      795\n",
      "top            NaN      M    student    55414\n",
      "freq           NaN    670        196        9\n",
      "mean     34.051962    NaN        NaN      NaN\n",
      "std      12.192740    NaN        NaN      NaN\n",
      "min       7.000000    NaN        NaN      NaN\n",
      "25%      25.000000    NaN        NaN      NaN\n",
      "50%      31.000000    NaN        NaN      NaN\n",
      "75%      43.000000    NaN        NaN      NaN\n",
      "max      73.000000    NaN        NaN      NaN\n",
      "\n",
      "Summary of occupation column:\n",
      "count         943\n",
      "unique         21\n",
      "top       student\n",
      "freq          196\n",
      "Name: occupation, dtype: object\n",
      "\n",
      "Mean age of users: 34.05196182396607\n",
      "\n",
      "Age with least occurrence:\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user\"\n",
    "users = pd.read_csv(url, sep='|')\n",
    "\n",
    "# Step 3. Assign it to a variable called users and use the 'user_id' as index\n",
    "users = users.set_index('user_id')\n",
    "\n",
    "# Step 4. See the first 25 entries\n",
    "print(\"First 25 entries:\")\n",
    "print(users.head(25))\n",
    "\n",
    "# Step 5. See the last 10 entries\n",
    "print(\"\\nLast 10 entries:\")\n",
    "print(users.tail(10))\n",
    "\n",
    "# Step 6. What is the number of observations in the dataset?\n",
    "print(\"\\nNumber of observations:\", len(users))\n",
    "\n",
    "# Step 7. What is the number of columns in the dataset?\n",
    "print(\"\\nNumber of columns:\", len(users.columns))\n",
    "\n",
    "# Step 8. Print the name of all the columns\n",
    "print(\"\\nColumn names:\")\n",
    "print(users.columns)\n",
    "\n",
    "# Step 9. How is the dataset indexed?\n",
    "print(\"\\nDataset indexing:\")\n",
    "print(users.index)\n",
    "\n",
    "# Step 10. What is the data type of each column?\n",
    "print(\"\\nData types of each column:\")\n",
    "print(users.dtypes)\n",
    "\n",
    "# Step 11. Print only the occupation column\n",
    "print(\"\\nOccupation column:\")\n",
    "print(users['occupation'])\n",
    "\n",
    "# Step 12. How many different occupations are in this dataset?\n",
    "print(\"\\nNumber of different occupations:\", users['occupation'].nunique())\n",
    "\n",
    "# Step 13. What is the most frequent occupation?\n",
    "print(\"\\nMost frequent occupation:\")\n",
    "print(users['occupation'].value_counts().idxmax())\n",
    "\n",
    "# Step 14. Summarize the DataFrame\n",
    "print(\"\\nDataFrame summary:\")\n",
    "print(users.describe())\n",
    "\n",
    "# Step 15. Summarize all the columns\n",
    "print(\"\\nSummary of all columns:\")\n",
    "print(users.describe(include='all'))\n",
    "\n",
    "# Step 16. Summarize only the occupation column\n",
    "print(\"\\nSummary of occupation column:\")\n",
    "print(users['occupation'].describe())\n",
    "\n",
    "# Step 17. What is the mean age of users?\n",
    "print(\"\\nMean age of users:\", users['age'].mean())\n",
    "\n",
    "# Step 18. What is the age with the least occurrence?\n",
    "print(\"\\nAge with least occurrence:\")\n",
    "print(users['age'].value_counts().idxmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c68b54",
   "metadata": {},
   "source": [
    "# Ex - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e29f5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_2324\\649318825.py:8: DtypeWarning: Columns (0,3,5,19,20,24,25,26,27,28,36,37,38,39,48) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  food = pd.read_csv('C:\\\\Users\\\\HP\\\\Downloads\\\\food_data.tsv',sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries:\n",
      "    code                                                url  \\\n",
      "0   3087  http://world-en.openfoodfacts.org/product/0000...   \n",
      "1   4530  http://world-en.openfoodfacts.org/product/0000...   \n",
      "2   4559  http://world-en.openfoodfacts.org/product/0000...   \n",
      "3  16087  http://world-en.openfoodfacts.org/product/0000...   \n",
      "4  16094  http://world-en.openfoodfacts.org/product/0000...   \n",
      "\n",
      "                      creator   created_t      created_datetime  \\\n",
      "0  openfoodfacts-contributors  1474103866  2016-09-17T09:17:46Z   \n",
      "1             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
      "2             usda-ndb-import  1489069957  2017-03-09T14:32:37Z   \n",
      "3             usda-ndb-import  1489055731  2017-03-09T10:35:31Z   \n",
      "4             usda-ndb-import  1489055653  2017-03-09T10:34:13Z   \n",
      "\n",
      "  last_modified_t last_modified_datetime                    product_name  \\\n",
      "0      1474103893   2016-09-17T09:18:13Z              Farine de blé noir   \n",
      "1      1489069957   2017-03-09T14:32:37Z  Banana Chips Sweetened (Whole)   \n",
      "2      1489069957   2017-03-09T14:32:37Z                         Peanuts   \n",
      "3      1489055731   2017-03-09T10:35:31Z          Organic Salted Nut Mix   \n",
      "4      1489055653   2017-03-09T10:34:13Z                 Organic Polenta   \n",
      "\n",
      "  generic_name quantity  ... fruits-vegetables-nuts_100g  \\\n",
      "0          NaN      1kg  ...                         NaN   \n",
      "1          NaN      NaN  ...                         NaN   \n",
      "2          NaN      NaN  ...                         NaN   \n",
      "3          NaN      NaN  ...                         NaN   \n",
      "4          NaN      NaN  ...                         NaN   \n",
      "\n",
      "  fruits-vegetables-nuts-estimate_100g collagen-meat-protein-ratio_100g  \\\n",
      "0                                  NaN                              NaN   \n",
      "1                                  NaN                              NaN   \n",
      "2                                  NaN                              NaN   \n",
      "3                                  NaN                              NaN   \n",
      "4                                  NaN                              NaN   \n",
      "\n",
      "  cocoa_100g chlorophyl_100g carbon-footprint_100g nutrition-score-fr_100g  \\\n",
      "0        NaN             NaN                   NaN                     NaN   \n",
      "1        NaN             NaN                   NaN                    14.0   \n",
      "2        NaN             NaN                   NaN                     0.0   \n",
      "3        NaN             NaN                   NaN                    12.0   \n",
      "4        NaN             NaN                   NaN                     NaN   \n",
      "\n",
      "  nutrition-score-uk_100g glycemic-index_100g water-hardness_100g  \n",
      "0                     NaN                 NaN                 NaN  \n",
      "1                    14.0                 NaN                 NaN  \n",
      "2                     0.0                 NaN                 NaN  \n",
      "3                    12.0                 NaN                 NaN  \n",
      "4                     NaN                 NaN                 NaN  \n",
      "\n",
      "[5 rows x 163 columns]\n",
      "\n",
      "Number of observations: 356027\n",
      "\n",
      "Number of columns: 163\n",
      "\n",
      "Column names:\n",
      "Index(['code', 'url', 'creator', 'created_t', 'created_datetime',\n",
      "       'last_modified_t', 'last_modified_datetime', 'product_name',\n",
      "       'generic_name', 'quantity',\n",
      "       ...\n",
      "       'fruits-vegetables-nuts_100g', 'fruits-vegetables-nuts-estimate_100g',\n",
      "       'collagen-meat-protein-ratio_100g', 'cocoa_100g', 'chlorophyl_100g',\n",
      "       'carbon-footprint_100g', 'nutrition-score-fr_100g',\n",
      "       'nutrition-score-uk_100g', 'glycemic-index_100g',\n",
      "       'water-hardness_100g'],\n",
      "      dtype='object', length=163)\n",
      "\n",
      "Name of the 105th column: -glucose_100g\n",
      "\n",
      "Type of observations in the 105th column: float64\n",
      "\n",
      "Dataset indexing:\n",
      "RangeIndex(start=0, stop=356027, step=1)\n",
      "\n",
      "Product name of the 19th observation: Lotus Organic Brown Jasmine Rice\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Go to https://www.kaggle.com/openfoodfacts/worldfood-facts/data\n",
    "# Step 2. Download the dataset to your computer and unzip it.\n",
    "# Step 3. Use the tsv file and assign it to a dataframe called food\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the file is named 'food_data.tsv' and located in the current directory\n",
    "file_path = \"food_data.tsv\"\n",
    "food = pd.read_csv('C:\\\\Users\\\\HP\\\\Downloads\\\\food_data.tsv',sep='\\t')\n",
    "\n",
    "# Step 4. See the first 5 entries\n",
    "print(\"First 5 entries:\")\n",
    "print(food.head())\n",
    "\n",
    "# Step 5. What is the number of observations in the dataset?\n",
    "print(\"\\nNumber of observations:\", len(food))\n",
    "\n",
    "# Step 6. What is the number of columns in the dataset?\n",
    "print(\"\\nNumber of columns:\", len(food.columns))\n",
    "\n",
    "# Step 7. Print the name of all the columns.\n",
    "print(\"\\nColumn names:\")\n",
    "print(food.columns)\n",
    "\n",
    "# Step 8. What is the name of 105th column?\n",
    "column_105_name = food.columns[104]  # Since Python is zero-indexed\n",
    "print(\"\\nName of the 105th column:\", column_105_name)\n",
    "\n",
    "# Step 9. What is the type of the observations of the 105th column?\n",
    "column_105_type = food.dtypes[104]  # Again, Python is zero-indexed\n",
    "print(\"\\nType of observations in the 105th column:\", column_105_type)\n",
    "\n",
    "# Step 10. How is the dataset indexed?\n",
    "print(\"\\nDataset indexing:\")\n",
    "print(food.index)\n",
    "\n",
    "# Step 11. What is the product name of the 19th observation?\n",
    "product_name_19th = food.loc[18, 'product_name']  # Since Python is zero-indexed\n",
    "print(\"\\nProduct name of the 19th observation:\", product_name_19th)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d9f32",
   "metadata": {},
   "source": [
    "# Ex1 - Filtering and Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1cd5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products costing more than $10.00: 1130\n",
      "\n",
      "Item prices:\n",
      "                                  item_name  item_price\n",
      "0              Chips and Fresh Tomato Salsa        2.39\n",
      "1                                      Izze        3.39\n",
      "2                          Nantucket Nectar        3.39\n",
      "3     Chips and Tomatillo-Green Chili Salsa        2.39\n",
      "4                              Chicken Bowl       16.98\n",
      "...                                     ...         ...\n",
      "4237                    Chips and Guacamole        8.50\n",
      "4354                       Steak Soft Tacos       18.50\n",
      "4489                    Chips and Guacamole       17.80\n",
      "4509                                  Chips        1.99\n",
      "4510                          Barbacoa Bowl       11.49\n",
      "\n",
      "[209 rows x 2 columns]\n",
      "\n",
      "Quantity of the most expensive item ordered: 15\n",
      "\n",
      "Number of times a Veggie Salad Bowl was ordered: 18\n",
      "\n",
      "Number of times someone ordered more than one Canned Soda: 20\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\"\n",
    "# Step 3. Assign it to a variable called chipo\n",
    "chipo = pd.read_csv(url, sep='\\t')\n",
    "\n",
    "# Step 4. How many products cost more than $10.00?\n",
    "chipo['item_price'] = chipo['item_price'].apply(lambda x: float(x[1:]))\n",
    "products_over_10 = chipo[chipo['item_price'] > 10]\n",
    "num_products_over_10 = len(products_over_10)\n",
    "print(\"Number of products costing more than $10.00:\", num_products_over_10)\n",
    "\n",
    "# Step 5. What is the price of each item? Print a DataFrame with only two columns item_name and item_price\n",
    "item_prices = chipo[['item_name', 'item_price']].drop_duplicates()\n",
    "print(\"\\nItem prices:\")\n",
    "print(item_prices)\n",
    "\n",
    "# Step 6. Sort by the name of the item\n",
    "sorted_chipo = chipo.sort_values(by='item_name')\n",
    "\n",
    "# Step 7. What was the quantity of the most expensive item ordered?\n",
    "most_expensive_item = chipo.loc[chipo['item_price'].idxmax()]\n",
    "print(\"\\nQuantity of the most expensive item ordered:\", most_expensive_item['quantity'])\n",
    "\n",
    "# Step 8. How many times was a Veggie Salad Bowl ordered?\n",
    "veggie_salad_bowl_orders = chipo[chipo['item_name'] == 'Veggie Salad Bowl']\n",
    "num_veggie_salad_bowl_orders = len(veggie_salad_bowl_orders)\n",
    "print(\"\\nNumber of times a Veggie Salad Bowl was ordered:\", num_veggie_salad_bowl_orders)\n",
    "\n",
    "# Step 9. How many times did someone order more than one Canned Soda?\n",
    "more_than_one_canned_soda_orders = chipo[(chipo['item_name'] == 'Canned Soda') & (chipo['quantity'] > 1)]\n",
    "num_more_than_one_canned_soda_orders = len(more_than_one_canned_soda_orders)\n",
    "print(\"\\nNumber of times someone ordered more than one Canned Soda:\", num_more_than_one_canned_soda_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823e22a",
   "metadata": {},
   "source": [
    "# Ex2 - Filtering and Sorting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d029170e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goals:\n",
      " 0      4\n",
      "1      4\n",
      "2      4\n",
      "3      5\n",
      "4      3\n",
      "5     10\n",
      "6      5\n",
      "7      6\n",
      "8      2\n",
      "9      2\n",
      "10     6\n",
      "11     1\n",
      "12     5\n",
      "13    12\n",
      "14     5\n",
      "15     2\n",
      "Name: Goals, dtype: int64\n",
      "\n",
      "Number of teams: 16\n",
      "\n",
      "Number of columns: 35\n",
      "\n",
      "Discipline:\n",
      "                    Team  Yellow Cards  Red Cards\n",
      "0               Croatia             9          0\n",
      "1        Czech Republic             7          0\n",
      "2               Denmark             4          0\n",
      "3               England             5          0\n",
      "4                France             6          0\n",
      "5               Germany             4          0\n",
      "6                Greece             9          1\n",
      "7                 Italy            16          0\n",
      "8           Netherlands             5          0\n",
      "9                Poland             7          1\n",
      "10             Portugal            12          0\n",
      "11  Republic of Ireland             6          1\n",
      "12               Russia             6          0\n",
      "13                Spain            11          0\n",
      "14               Sweden             7          0\n",
      "15              Ukraine             5          0\n",
      "\n",
      "Discipline Sorted:\n",
      "                    Team  Yellow Cards  Red Cards\n",
      "6                Greece             9          1\n",
      "9                Poland             7          1\n",
      "11  Republic of Ireland             6          1\n",
      "7                 Italy            16          0\n",
      "10             Portugal            12          0\n",
      "13                Spain            11          0\n",
      "0               Croatia             9          0\n",
      "1        Czech Republic             7          0\n",
      "14               Sweden             7          0\n",
      "4                France             6          0\n",
      "12               Russia             6          0\n",
      "3               England             5          0\n",
      "8           Netherlands             5          0\n",
      "15              Ukraine             5          0\n",
      "2               Denmark             4          0\n",
      "5               Germany             4          0\n",
      "\n",
      "Mean Yellow Cards per Team: 7.4375\n",
      "\n",
      "High Scoring Teams:\n",
      "        Team  Goals  Shots on target  Shots off target Shooting Accuracy  \\\n",
      "5   Germany     10               32                32             47.8%   \n",
      "13    Spain     12               42                33             55.9%   \n",
      "\n",
      "   % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  \\\n",
      "5             15.6%                          80             2              1   \n",
      "13            16.0%                         100             0              1   \n",
      "\n",
      "    Penalties not scored  ...  Saves made  Saves-to-shots ratio  Fouls Won  \\\n",
      "5                      0  ...          10                 62.6%         63   \n",
      "13                     0  ...          15                 93.8%        102   \n",
      "\n",
      "   Fouls Conceded  Offsides  Yellow Cards  Red Cards  Subs on  Subs off  \\\n",
      "5              49        12             4          0       15        15   \n",
      "13             83        19            11          0       17        17   \n",
      "\n",
      "    Players Used  \n",
      "5             17  \n",
      "13            18  \n",
      "\n",
      "[2 rows x 35 columns]\n",
      "\n",
      "Teams Starting with 'G':\n",
      "       Team  Goals  Shots on target  Shots off target Shooting Accuracy  \\\n",
      "5  Germany     10               32                32             47.8%   \n",
      "6   Greece      5                8                18             30.7%   \n",
      "\n",
      "  % Goals-to-shots  Total shots (inc. Blocked)  Hit Woodwork  Penalty goals  \\\n",
      "5            15.6%                          80             2              1   \n",
      "6            19.2%                          32             1              1   \n",
      "\n",
      "   Penalties not scored  ...  Saves made  Saves-to-shots ratio  Fouls Won  \\\n",
      "5                     0  ...          10                 62.6%         63   \n",
      "6                     1  ...          13                 65.1%         67   \n",
      "\n",
      "  Fouls Conceded  Offsides  Yellow Cards  Red Cards  Subs on  Subs off  \\\n",
      "5             49        12             4          0       15        15   \n",
      "6             48        12             9          1       12        12   \n",
      "\n",
      "   Players Used  \n",
      "5            17  \n",
      "6            20  \n",
      "\n",
      "[2 rows x 35 columns]\n",
      "\n",
      "First 7 Columns:\n",
      "                    Team  Goals  Shots on target  Shots off target  \\\n",
      "0               Croatia      4               13                12   \n",
      "1        Czech Republic      4               13                18   \n",
      "2               Denmark      4               10                10   \n",
      "3               England      5               11                18   \n",
      "4                France      3               22                24   \n",
      "5               Germany     10               32                32   \n",
      "6                Greece      5                8                18   \n",
      "7                 Italy      6               34                45   \n",
      "8           Netherlands      2               12                36   \n",
      "9                Poland      2               15                23   \n",
      "10             Portugal      6               22                42   \n",
      "11  Republic of Ireland      1                7                12   \n",
      "12               Russia      5                9                31   \n",
      "13                Spain     12               42                33   \n",
      "14               Sweden      5               17                19   \n",
      "15              Ukraine      2                7                26   \n",
      "\n",
      "   Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  \n",
      "0              51.9%            16.0%                          32  \n",
      "1              41.9%            12.9%                          39  \n",
      "2              50.0%            20.0%                          27  \n",
      "3              50.0%            17.2%                          40  \n",
      "4              37.9%             6.5%                          65  \n",
      "5              47.8%            15.6%                          80  \n",
      "6              30.7%            19.2%                          32  \n",
      "7              43.0%             7.5%                         110  \n",
      "8              25.0%             4.1%                          60  \n",
      "9              39.4%             5.2%                          48  \n",
      "10             34.3%             9.3%                          82  \n",
      "11             36.8%             5.2%                          28  \n",
      "12             22.5%            12.5%                          59  \n",
      "13             55.9%            16.0%                         100  \n",
      "14             47.2%            13.8%                          39  \n",
      "15             21.2%             6.0%                          38  \n",
      "\n",
      "All Columns Except Last 3:\n",
      "                    Team  Goals  Shots on target  Shots off target  \\\n",
      "0               Croatia      4               13                12   \n",
      "1        Czech Republic      4               13                18   \n",
      "2               Denmark      4               10                10   \n",
      "3               England      5               11                18   \n",
      "4                France      3               22                24   \n",
      "5               Germany     10               32                32   \n",
      "6                Greece      5                8                18   \n",
      "7                 Italy      6               34                45   \n",
      "8           Netherlands      2               12                36   \n",
      "9                Poland      2               15                23   \n",
      "10             Portugal      6               22                42   \n",
      "11  Republic of Ireland      1                7                12   \n",
      "12               Russia      5                9                31   \n",
      "13                Spain     12               42                33   \n",
      "14               Sweden      5               17                19   \n",
      "15              Ukraine      2                7                26   \n",
      "\n",
      "   Shooting Accuracy % Goals-to-shots  Total shots (inc. Blocked)  \\\n",
      "0              51.9%            16.0%                          32   \n",
      "1              41.9%            12.9%                          39   \n",
      "2              50.0%            20.0%                          27   \n",
      "3              50.0%            17.2%                          40   \n",
      "4              37.9%             6.5%                          65   \n",
      "5              47.8%            15.6%                          80   \n",
      "6              30.7%            19.2%                          32   \n",
      "7              43.0%             7.5%                         110   \n",
      "8              25.0%             4.1%                          60   \n",
      "9              39.4%             5.2%                          48   \n",
      "10             34.3%             9.3%                          82   \n",
      "11             36.8%             5.2%                          28   \n",
      "12             22.5%            12.5%                          59   \n",
      "13             55.9%            16.0%                         100   \n",
      "14             47.2%            13.8%                          39   \n",
      "15             21.2%             6.0%                          38   \n",
      "\n",
      "    Hit Woodwork  Penalty goals  Penalties not scored  ...  Clean Sheets  \\\n",
      "0              0              0                     0  ...             0   \n",
      "1              0              0                     0  ...             1   \n",
      "2              1              0                     0  ...             1   \n",
      "3              0              0                     0  ...             2   \n",
      "4              1              0                     0  ...             1   \n",
      "5              2              1                     0  ...             1   \n",
      "6              1              1                     1  ...             1   \n",
      "7              2              0                     0  ...             2   \n",
      "8              2              0                     0  ...             0   \n",
      "9              0              0                     0  ...             0   \n",
      "10             6              0                     0  ...             2   \n",
      "11             0              0                     0  ...             0   \n",
      "12             2              0                     0  ...             0   \n",
      "13             0              1                     0  ...             5   \n",
      "14             3              0                     0  ...             1   \n",
      "15             0              0                     0  ...             0   \n",
      "\n",
      "    Blocks  Goals conceded Saves made  Saves-to-shots ratio  Fouls Won  \\\n",
      "0       10               3         13                 81.3%         41   \n",
      "1       10               6          9                 60.1%         53   \n",
      "2       10               5         10                 66.7%         25   \n",
      "3       29               3         22                 88.1%         43   \n",
      "4        7               5          6                 54.6%         36   \n",
      "5       11               6         10                 62.6%         63   \n",
      "6       23               7         13                 65.1%         67   \n",
      "7       18               7         20                 74.1%        101   \n",
      "8        9               5         12                 70.6%         35   \n",
      "9        8               3          6                 66.7%         48   \n",
      "10      11               4         10                 71.5%         73   \n",
      "11      23               9         17                 65.4%         43   \n",
      "12       8               3         10                 77.0%         34   \n",
      "13       8               1         15                 93.8%        102   \n",
      "14      12               5          8                 61.6%         35   \n",
      "15       4               4         13                 76.5%         48   \n",
      "\n",
      "    Fouls Conceded  Offsides  Yellow Cards  Red Cards  \n",
      "0               62         2             9          0  \n",
      "1               73         8             7          0  \n",
      "2               38         8             4          0  \n",
      "3               45         6             5          0  \n",
      "4               51         5             6          0  \n",
      "5               49        12             4          0  \n",
      "6               48        12             9          1  \n",
      "7               89        16            16          0  \n",
      "8               30         3             5          0  \n",
      "9               56         3             7          1  \n",
      "10              90        10            12          0  \n",
      "11              51        11             6          1  \n",
      "12              43         4             6          0  \n",
      "13              83        19            11          0  \n",
      "14              51         7             7          0  \n",
      "15              31         4             5          0  \n",
      "\n",
      "[16 rows x 32 columns]\n",
      "\n",
      "Shooting Accuracy for England, Italy, and Russia:\n",
      "        Team Shooting Accuracy\n",
      "3   England             50.0%\n",
      "7     Italy             43.0%\n",
      "12   Russia             22.5%\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv\"\n",
    "\n",
    "# Step 3. Assign it to a variable called euro12\n",
    "euro12 = pd.read_csv(url)\n",
    "euro12\n",
    "\n",
    "# Step 4. Select only the Goal column\n",
    "goals = euro12['Goals']\n",
    "\n",
    "# Step 5. How many teams participated in the Euro2012?\n",
    "num_teams = euro12['Team'].nunique()\n",
    "\n",
    "# Step 6. What is the number of columns in the dataset?\n",
    "num_columns = euro12.shape[1]\n",
    "\n",
    "# Step 7. View only the columns Team, Yellow Cards, and Red Cards and assign them to a dataframe called discipline\n",
    "discipline = euro12[['Team', 'Yellow Cards', 'Red Cards']]\n",
    "\n",
    "# Step 8. Sort the teams by Red Cards, then by Yellow Cards\n",
    "discipline_sorted = discipline.sort_values(by=['Red Cards', 'Yellow Cards'], ascending=False)\n",
    "\n",
    "# Step 9. Calculate the mean Yellow Cards given per Team\n",
    "mean_yellow_cards = discipline['Yellow Cards'].mean()\n",
    "\n",
    "# Step 10. Filter teams that scored more than 6 goals\n",
    "high_scoring_teams = euro12[euro12['Goals'] > 6]\n",
    "\n",
    "# Step 11. Select the teams that start with G\n",
    "teams_starting_with_G = euro12[euro12['Team'].str.startswith('G')]\n",
    "\n",
    "# Step 12. Select the first 7 columns\n",
    "first_7_columns = euro12.iloc[:, :7]\n",
    "\n",
    "# Step 13. Select all columns except the last 3\n",
    "all_except_last_3_columns = euro12.iloc[:, :-3]\n",
    "\n",
    "# Step 14. Present only the Shooting Accuracy from England, Italy, and Russia\n",
    "shooting_accuracy = euro12.loc[euro12['Team'].isin(['England', 'Italy', 'Russia']), ['Team', 'Shooting Accuracy']]\n",
    "\n",
    "# Step 15. Print all objects\n",
    "print(\"Goals:\\n\", goals)\n",
    "print(\"\\nNumber of teams:\", num_teams)\n",
    "print(\"\\nNumber of columns:\", num_columns)\n",
    "print(\"\\nDiscipline:\\n\", discipline)\n",
    "print(\"\\nDiscipline Sorted:\\n\", discipline_sorted)\n",
    "print(\"\\nMean Yellow Cards per Team:\", mean_yellow_cards)\n",
    "print(\"\\nHigh Scoring Teams:\\n\", high_scoring_teams)\n",
    "print(\"\\nTeams Starting with 'G':\\n\", teams_starting_with_G)\n",
    "print(\"\\nFirst 7 Columns:\\n\", first_7_columns)\n",
    "print(\"\\nAll Columns Except Last 3:\\n\", all_except_last_3_columns)\n",
    "print(\"\\nShooting Accuracy for England, Italy, and Russia:\\n\", shooting_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ec0bd",
   "metadata": {},
   "source": [
    "# Ex3 - Fictional Army - Filtering and Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62853616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "origin\n",
      "Arizona         1\n",
      "California      5\n",
      "Texas          62\n",
      "Florida        26\n",
      "Maine          73\n",
      "Iowa           37\n",
      "Alaska        949\n",
      "Washington     48\n",
      "Oregon         48\n",
      "Wyoming       435\n",
      "Louisana       63\n",
      "Georgia       345\n",
      "Name: veterans, dtype: int64\n",
      "            veterans  deaths\n",
      "origin                      \n",
      "Arizona            1     523\n",
      "California         5      52\n",
      "Texas             62      25\n",
      "Florida           26     616\n",
      "Maine             73      43\n",
      "Iowa              37     234\n",
      "Alaska           949     523\n",
      "Washington        48      62\n",
      "Oregon            48      62\n",
      "Wyoming          435      73\n",
      "Louisana          63      37\n",
      "Georgia          345      35\n",
      "Index(['regiment', 'company', 'deaths', 'battles', 'size', 'veterans',\n",
      "       'readiness', 'armored', 'deserters'],\n",
      "      dtype='object')\n",
      "        deaths  size  deserters\n",
      "origin                         \n",
      "Maine       43  1592          3\n",
      "Alaska     523   987         24\n",
      "         deaths  battles  size  veterans\n",
      "origin                                  \n",
      "Texas        25        2  1099        62\n",
      "Florida     616        2  1400        26\n",
      "Maine        43        4  1592        73\n",
      "Iowa        234        7  1006        37\n",
      "Alaska      523        8   987       949\n",
      "            regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                     \n",
      "Maine       Dragoons     1st      43        4  1592        73          2   \n",
      "Iowa        Dragoons     1st     234        7  1006        37          1   \n",
      "Alaska      Dragoons     2nd     523        8   987       949          2   \n",
      "Washington  Dragoons     2nd      62        3   849        48          3   \n",
      "Oregon        Scouts     1st      62        4   973        48          2   \n",
      "Wyoming       Scouts     1st      73        7  1005       435          1   \n",
      "Louisana      Scouts     2nd      37        8  1099        63          2   \n",
      "Georgia       Scouts     2nd      35        9  1523       345          3   \n",
      "\n",
      "            armored  deserters  \n",
      "origin                          \n",
      "Maine             0          3  \n",
      "Iowa              1          4  \n",
      "Alaska            0         24  \n",
      "Washington        1         31  \n",
      "Oregon            0          2  \n",
      "Wyoming           0          3  \n",
      "Louisana          1          2  \n",
      "Georgia           1          3  \n",
      "              regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                       \n",
      "Arizona     Nighthawks     1st     523        5  1045         1          1   \n",
      "California  Nighthawks     1st      52       42   957         5          2   \n",
      "Texas       Nighthawks     2nd      25        2  1099        62          3   \n",
      "Florida     Nighthawks     2nd     616        2  1400        26          3   \n",
      "\n",
      "            armored  deserters  \n",
      "origin                          \n",
      "Arizona           1          4  \n",
      "California        0         24  \n",
      "Texas             1         31  \n",
      "Florida           1          2  \n",
      "            deaths  battles  size  veterans  readiness\n",
      "origin                                                \n",
      "Arizona        523        5  1045         1          1\n",
      "California      52       42   957         5          2\n",
      "Texas           25        2  1099        62          3\n",
      "Florida        616        2  1400        26          3\n",
      "Maine           43        4  1592        73          2\n",
      "Iowa           234        7  1006        37          1\n",
      "Alaska         523        8   987       949          2\n",
      "Washington      62        3   849        48          3\n",
      "Oregon          62        4   973        48          2\n",
      "Wyoming         73        7  1005       435          1\n",
      "Louisana        37        8  1099        63          2\n",
      "Georgia         35        9  1523       345          3\n",
      "              regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                       \n",
      "Arizona     Nighthawks     1st     523        5  1045         1          1   \n",
      "California  Nighthawks     1st      52       42   957         5          2   \n",
      "Florida     Nighthawks     2nd     616        2  1400        26          3   \n",
      "Iowa          Dragoons     1st     234        7  1006        37          1   \n",
      "Alaska        Dragoons     2nd     523        8   987       949          2   \n",
      "Washington    Dragoons     2nd      62        3   849        48          3   \n",
      "Oregon          Scouts     1st      62        4   973        48          2   \n",
      "Wyoming         Scouts     1st      73        7  1005       435          1   \n",
      "\n",
      "            armored  deserters  \n",
      "origin                          \n",
      "Arizona           1          4  \n",
      "California        0         24  \n",
      "Florida           1          2  \n",
      "Iowa              1          4  \n",
      "Alaska            0         24  \n",
      "Washington        1         31  \n",
      "Oregon            0          2  \n",
      "Wyoming           0          3  \n",
      "            regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                     \n",
      "Arizona   Nighthawks     1st     523        5  1045         1          1   \n",
      "Texas     Nighthawks     2nd      25        2  1099        62          3   \n",
      "Florida   Nighthawks     2nd     616        2  1400        26          3   \n",
      "Maine       Dragoons     1st      43        4  1592        73          2   \n",
      "Alaska      Dragoons     2nd     523        8   987       949          2   \n",
      "Louisana      Scouts     2nd      37        8  1099        63          2   \n",
      "Georgia       Scouts     2nd      35        9  1523       345          3   \n",
      "\n",
      "          armored  deserters  \n",
      "origin                        \n",
      "Arizona         1          4  \n",
      "Texas           1         31  \n",
      "Florida         1          2  \n",
      "Maine           0          3  \n",
      "Alaska          0         24  \n",
      "Louisana        1          2  \n",
      "Georgia         1          3  \n",
      "              regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                       \n",
      "Arizona     Nighthawks     1st     523        5  1045         1          1   \n",
      "California  Nighthawks     1st      52       42   957         5          2   \n",
      "Texas       Nighthawks     2nd      25        2  1099        62          3   \n",
      "Florida     Nighthawks     2nd     616        2  1400        26          3   \n",
      "Oregon          Scouts     1st      62        4   973        48          2   \n",
      "Wyoming         Scouts     1st      73        7  1005       435          1   \n",
      "Louisana        Scouts     2nd      37        8  1099        63          2   \n",
      "Georgia         Scouts     2nd      35        9  1523       345          3   \n",
      "\n",
      "            armored  deserters  \n",
      "origin                          \n",
      "Arizona           1          4  \n",
      "California        0         24  \n",
      "Texas             1         31  \n",
      "Florida           1          2  \n",
      "Oregon            0          2  \n",
      "Wyoming           0          3  \n",
      "Louisana          1          2  \n",
      "Georgia           1          3  \n",
      "           regiment company  deaths  battles  size  veterans  readiness  \\\n",
      "origin                                                                    \n",
      "Texas    Nighthawks     2nd      25        2  1099        62          3   \n",
      "Arizona  Nighthawks     1st     523        5  1045         1          1   \n",
      "\n",
      "         armored  deserters  \n",
      "origin                       \n",
      "Texas          1         31  \n",
      "Arizona        1          4  \n",
      "regiment     Nighthawks\n",
      "company             1st\n",
      "deaths              523\n",
      "battles               5\n",
      "size               1045\n",
      "veterans              1\n",
      "readiness             1\n",
      "armored               1\n",
      "deserters             4\n",
      "Name: Arizona, dtype: object\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. This is the data given as a dictionary\n",
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "            'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'],\n",
    "            'deaths': [523, 52, 25, 616, 43, 234, 523, 62, 62, 73, 37, 35],\n",
    "            'battles': [5, 42, 2, 2, 4, 7, 8, 3, 4, 7, 8, 9],\n",
    "            'size': [1045, 957, 1099, 1400, 1592, 1006, 987, 849, 973, 1005, 1099, 1523],\n",
    "            'veterans': [1, 5, 62, 26, 73, 37, 949, 48, 48, 435, 63, 345],\n",
    "            'readiness': [1, 2, 3, 3, 2, 1, 2, 3, 2, 1, 2, 3],\n",
    "            'armored': [1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
    "            'deserters': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "            'origin': ['Arizona', 'California', 'Texas', 'Florida', 'Maine', 'Iowa', 'Alaska', 'Washington', 'Oregon', 'Wyoming', 'Louisana', 'Georgia']}\n",
    "\n",
    "# Step 3. Create a dataframe and assign it to a variable called army\n",
    "army = pd.DataFrame(raw_data)\n",
    "\n",
    "# Step 4. Set the 'origin' column as the index of the dataframe\n",
    "army.set_index('origin', inplace=True)\n",
    "\n",
    "# Step 5. Print only the column veterans\n",
    "print(army['veterans'])\n",
    "\n",
    "# Step 6. Print the columns 'veterans' and 'deaths'\n",
    "print(army[['veterans', 'deaths']])\n",
    "\n",
    "# Step 7. Print the name of all the columns\n",
    "print(army.columns)\n",
    "\n",
    "# Step 8. Select the 'deaths', 'size' and 'deserters' columns from Maine and Alaska\n",
    "print(army.loc[['Maine', 'Alaska'], ['deaths', 'size', 'deserters']])\n",
    "\n",
    "# Step 9. Select the rows 3 to 7 and the columns 3 to 6\n",
    "print(army.iloc[2:7, 2:6])\n",
    "\n",
    "# Step 10. Select every row after the fourth row and all columns\n",
    "print(army.iloc[4:])\n",
    "\n",
    "# Step 11. Select every row up to the 4th row and all columns\n",
    "print(army.iloc[:4])\n",
    "\n",
    "# Step 12. Select the 3rd column up to the 7th column\n",
    "print(army.iloc[:, 2:7])\n",
    "\n",
    "# Step 13. Select rows where df.deaths is greater than 50\n",
    "print(army[army['deaths'] > 50])\n",
    "\n",
    "# Step 14. Select rows where df.deaths is greater than 500 or less than 50\n",
    "print(army[(army['deaths'] > 500) | (army['deaths'] < 50)])\n",
    "\n",
    "# Step 15. Select all the regiments not named \"Dragoons\"\n",
    "print(army[army['regiment'] != 'Dragoons'])\n",
    "\n",
    "# Step 16. Select the rows called Texas and Arizona\n",
    "print(army.loc[['Texas', 'Arizona']])\n",
    "\n",
    "# Step 17. Select the third cell in the row named Arizona\n",
    "print(army.loc['Arizona'])\n",
    "\n",
    "# Step 18. Select the third cell down in the column named deaths\n",
    "print(army.iloc[2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7a5fe",
   "metadata": {},
   "source": [
    "# Ex4 - GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "57bc959d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         country  beer_servings  spirit_servings  wine_servings  \\\n",
      "0    Afghanistan              0                0              0   \n",
      "1        Albania             89              132             54   \n",
      "2        Algeria             25                0             14   \n",
      "3        Andorra            245              138            312   \n",
      "4         Angola            217               57             45   \n",
      "..           ...            ...              ...            ...   \n",
      "188    Venezuela            333              100              3   \n",
      "189      Vietnam            111                2              1   \n",
      "190        Yemen              6                0              0   \n",
      "191       Zambia             32               19              4   \n",
      "192     Zimbabwe             64               18              4   \n",
      "\n",
      "     total_litres_of_pure_alcohol continent  \n",
      "0                             0.0        AS  \n",
      "1                             4.9        EU  \n",
      "2                             0.7        AF  \n",
      "3                            12.4        EU  \n",
      "4                             5.9        AF  \n",
      "..                            ...       ...  \n",
      "188                           7.7        SA  \n",
      "189                           2.0        AS  \n",
      "190                           0.1        AS  \n",
      "191                           2.5        AF  \n",
      "192                           4.7        AF  \n",
      "\n",
      "[193 rows x 6 columns]\n",
      "The continent that drinks more beer on average is: EU\n",
      "Statistics for wine consumption by continent:\n",
      "           count        mean        std  min   25%    50%     75%    max\n",
      "continent                                                               \n",
      "AF          53.0   16.264151  38.846419  0.0   1.0    2.0   13.00  233.0\n",
      "AS          44.0    9.068182  21.667034  0.0   0.0    1.0    8.00  123.0\n",
      "EU          45.0  142.222222  97.421738  0.0  59.0  128.0  195.00  370.0\n",
      "OC          16.0   35.625000  64.555790  0.0   1.0    8.5   23.25  212.0\n",
      "SA          12.0   62.416667  88.620189  1.0   3.0   12.0   98.50  221.0\n",
      "Mean alcohol consumption per continent for every column:\n",
      "continent\n",
      "AF    3.007547\n",
      "AS    2.170455\n",
      "EU    8.617778\n",
      "OC    3.381250\n",
      "SA    6.308333\n",
      "Name: total_litres_of_pure_alcohol, dtype: float64\n",
      "Median alcohol consumption per continent for every column:\n",
      "continent\n",
      "AF     2.30\n",
      "AS     1.20\n",
      "EU    10.00\n",
      "OC     1.75\n",
      "SA     6.85\n",
      "Name: total_litres_of_pure_alcohol, dtype: float64\n",
      "Mean, min, and max values for spirit consumption by continent:\n",
      "                 mean  min  max\n",
      "continent                      \n",
      "AF          16.339623    0  152\n",
      "AS          60.840909    0  326\n",
      "EU         132.555556    0  373\n",
      "OC          58.437500    0  254\n",
      "SA         114.750000   25  302\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from this address\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/drinks.csv'\n",
    "\n",
    "# Step 3. Assign it to a variable called drinks\n",
    "drinks = pd.read_csv(url)\n",
    "print(drinks)\n",
    "\n",
    "#Step 4.Which continent drinks more beer on average?\n",
    "beer_by_continent = drinks.groupby('continent')['beer_servings'].mean()\n",
    "continent_with_most_beer = beer_by_continent.idxmax()\n",
    "print(f\"The continent that drinks more beer on average is: {continent_with_most_beer}\")\n",
    "\n",
    "# Step 5. For each continent print the statistics for wine consumption\n",
    "wine_stats = drinks.groupby('continent')['wine_servings'].describe()\n",
    "print(\"Statistics for wine consumption by continent:\")\n",
    "print(wine_stats)\n",
    "\n",
    "# Step 6. Print the mean alcohol consumption per continent for every column\n",
    "mean_alcohol_by_continent = drinks.groupby('continent')['total_litres_of_pure_alcohol'].mean()\n",
    "print(\"Mean alcohol consumption per continent for every column:\")\n",
    "print(mean_alcohol_by_continent)\n",
    "\n",
    "#Step 7. Print the median alcohol consumption per continent for every column\n",
    "mean_alcohol_by_continent = drinks.groupby('continent')['total_litres_of_pure_alcohol'].median()\n",
    "print(\"Median alcohol consumption per continent for every column:\")\n",
    "print(mean_alcohol_by_continent)\n",
    "\n",
    "# Step 8. Print the mean, min and max values for spirit consumption. This time output a DataFrame\n",
    "spirit_stats = drinks.groupby('continent')['spirit_servings'].agg(['mean', 'min', 'max'])\n",
    "print(\"Mean, min, and max values for spirit consumption by continent:\")\n",
    "print(spirit_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b62c4",
   "metadata": {},
   "source": [
    "# Ex5 - Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e14b443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean age per occupation:\n",
      " occupation\n",
      "administrator    38.746835\n",
      "artist           31.392857\n",
      "doctor           43.571429\n",
      "educator         42.010526\n",
      "engineer         36.388060\n",
      "entertainment    29.222222\n",
      "executive        38.718750\n",
      "healthcare       41.562500\n",
      "homemaker        32.571429\n",
      "lawyer           36.750000\n",
      "librarian        40.000000\n",
      "marketing        37.615385\n",
      "none             26.555556\n",
      "other            34.523810\n",
      "programmer       33.121212\n",
      "retired          63.071429\n",
      "salesman         35.666667\n",
      "scientist        35.548387\n",
      "student          22.081633\n",
      "technician       33.148148\n",
      "writer           36.311111\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Male ratio per occupation (sorted):\n",
      " occupation\n",
      "administrator    0\n",
      "marketing        0\n",
      "technician       0\n",
      "student          0\n",
      "scientist        0\n",
      "salesman         0\n",
      "retired          0\n",
      "programmer       0\n",
      "other            0\n",
      "none             0\n",
      "librarian        0\n",
      "artist           0\n",
      "lawyer           0\n",
      "homemaker        0\n",
      "healthcare       0\n",
      "executive        0\n",
      "entertainment    0\n",
      "engineer         0\n",
      "educator         0\n",
      "doctor           0\n",
      "writer           0\n",
      "dtype: int64\n",
      "\n",
      "Minimum and maximum ages per occupation:\n",
      "                min  max\n",
      "occupation             \n",
      "administrator   21   70\n",
      "artist          19   48\n",
      "doctor          28   64\n",
      "educator        23   63\n",
      "engineer        22   70\n",
      "entertainment   15   50\n",
      "executive       22   69\n",
      "healthcare      22   62\n",
      "homemaker       20   50\n",
      "lawyer          21   53\n",
      "librarian       23   69\n",
      "marketing       24   55\n",
      "none            11   55\n",
      "other           13   64\n",
      "programmer      20   63\n",
      "retired         51   73\n",
      "salesman        18   66\n",
      "scientist       23   55\n",
      "student          7   42\n",
      "technician      21   55\n",
      "writer          18   60\n",
      "\n",
      "Mean age per occupation and gender:\n",
      " occupation     gender\n",
      "administrator  F         40.638889\n",
      "               M         37.162791\n",
      "artist         F         30.307692\n",
      "               M         32.333333\n",
      "doctor         M         43.571429\n",
      "educator       F         39.115385\n",
      "               M         43.101449\n",
      "engineer       F         29.500000\n",
      "               M         36.600000\n",
      "entertainment  F         31.000000\n",
      "               M         29.000000\n",
      "executive      F         44.000000\n",
      "               M         38.172414\n",
      "healthcare     F         39.818182\n",
      "               M         45.400000\n",
      "homemaker      F         34.166667\n",
      "               M         23.000000\n",
      "lawyer         F         39.500000\n",
      "               M         36.200000\n",
      "librarian      F         40.000000\n",
      "               M         40.000000\n",
      "marketing      F         37.200000\n",
      "               M         37.875000\n",
      "none           F         36.500000\n",
      "               M         18.600000\n",
      "other          F         35.472222\n",
      "               M         34.028986\n",
      "programmer     F         32.166667\n",
      "               M         33.216667\n",
      "retired        F         70.000000\n",
      "               M         62.538462\n",
      "salesman       F         27.000000\n",
      "               M         38.555556\n",
      "scientist      F         28.333333\n",
      "               M         36.321429\n",
      "student        F         20.750000\n",
      "               M         22.669118\n",
      "technician     F         38.000000\n",
      "               M         32.961538\n",
      "writer         F         37.631579\n",
      "               M         35.346154\n",
      "Name: age, dtype: float64\n",
      "\n",
      "Percentage of women and men per occupation:\n",
      " gender            F      M   % Female      % Male\n",
      "occupation                                       \n",
      "administrator  36.0   43.0  45.569620   54.430380\n",
      "artist         13.0   15.0  46.428571   53.571429\n",
      "doctor          0.0    7.0   0.000000  100.000000\n",
      "educator       26.0   69.0  27.368421   72.631579\n",
      "engineer        2.0   65.0   2.985075   97.014925\n",
      "entertainment   2.0   16.0  11.111111   88.888889\n",
      "executive       3.0   29.0   9.375000   90.625000\n",
      "healthcare     11.0    5.0  68.750000   31.250000\n",
      "homemaker       6.0    1.0  85.714286   14.285714\n",
      "lawyer          2.0   10.0  16.666667   83.333333\n",
      "librarian      29.0   22.0  56.862745   43.137255\n",
      "marketing      10.0   16.0  38.461538   61.538462\n",
      "none            4.0    5.0  44.444444   55.555556\n",
      "other          36.0   69.0  34.285714   65.714286\n",
      "programmer      6.0   60.0   9.090909   90.909091\n",
      "retired         1.0   13.0   7.142857   92.857143\n",
      "salesman        3.0    9.0  25.000000   75.000000\n",
      "scientist       3.0   28.0   9.677419   90.322581\n",
      "student        60.0  136.0  30.612245   69.387755\n",
      "technician      1.0   26.0   3.703704   96.296296\n",
      "writer         19.0   26.0  42.222222   57.777778\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/u.user\"\n",
    "users = pd.read_csv(url, delimiter='|', index_col='user_id')\n",
    "\n",
    "# Step 3. Assign it to a variable called users\n",
    "# This step is already done in the previous step\n",
    "\n",
    "# Step 4. Discover what is the mean age per occupation\n",
    "mean_age_per_occupation = users.groupby('occupation')['age'].mean()\n",
    "\n",
    "# Step 5. Discover the Male ratio per occupation and sort it from the most to the least\n",
    "def male_ratio(x):\n",
    "    if 'M' in x['gender']:\n",
    "        return (x['gender'] == 'M').sum() / x['gender'].count()\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "male_ratio_per_occupation = users.groupby('occupation').apply(male_ratio).sort_values(ascending=False)\n",
    "\n",
    "# Step 6. For each occupation, calculate the minimum and maximum ages\n",
    "min_max_age_per_occupation = users.groupby('occupation')['age'].agg(['min', 'max'])\n",
    "\n",
    "# Step 7. For each combination of occupation and gender, calculate the mean age\n",
    "mean_age_per_occupation_gender = users.groupby(['occupation', 'gender'])['age'].mean()\n",
    "\n",
    "# Step 8. For each occupation, present the percentage of women and men\n",
    "total_gender_per_occupation = users.groupby('occupation')['gender'].count()\n",
    "percentage_gender_per_occupation = users.groupby(['occupation', 'gender']).size().unstack().fillna(0)\n",
    "percentage_gender_per_occupation['% Female'] = (percentage_gender_per_occupation['F'] / total_gender_per_occupation) * 100\n",
    "percentage_gender_per_occupation['% Male'] = (percentage_gender_per_occupation['M'] / total_gender_per_occupation) * 100\n",
    "\n",
    "# Displaying the results\n",
    "print(\"Mean age per occupation:\\n\", mean_age_per_occupation)\n",
    "print(\"\\nMale ratio per occupation (sorted):\\n\", male_ratio_per_occupation)\n",
    "print(\"\\nMinimum and maximum ages per occupation:\\n\", min_max_age_per_occupation)\n",
    "print(\"\\nMean age per occupation and gender:\\n\", mean_age_per_occupation_gender)\n",
    "print(\"\\nPercentage of women and men per occupation:\\n\", percentage_gender_per_occupation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9048e542",
   "metadata": {},
   "source": [
    "# Ex6 - Regiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10dd21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regiment: Dragoons\n",
      "   regiment company    name  preTestScore  postTestScore\n",
      "4  Dragoons     1st   Cooze             3             70\n",
      "5  Dragoons     1st   Jacon             4             25\n",
      "6  Dragoons     2nd  Ryaner            24             94\n",
      "7  Dragoons     2nd    Sone            31             57\n",
      "Regiment: Nighthawks\n",
      "     regiment company      name  preTestScore  postTestScore\n",
      "0  Nighthawks     1st    Miller             4             25\n",
      "1  Nighthawks     1st  Jacobson            24             94\n",
      "2  Nighthawks     2nd       Ali            31             57\n",
      "3  Nighthawks     2nd    Milner             2             62\n",
      "Regiment: Scouts\n",
      "   regiment company   name  preTestScore  postTestScore\n",
      "8    Scouts     1st  Sloan             2             62\n",
      "9    Scouts     1st  Piger             3             70\n",
      "10   Scouts     2nd  Riani             2             62\n",
      "11   Scouts     2nd    Ali             3             70\n",
      "\n",
      "Mean preTestScore from the regiment Nighthawks: 15.25\n",
      "\n",
      "General statistics by company:\n",
      "         preTestScore                                                      \\\n",
      "               count       mean        std  min   25%   50%    75%   max   \n",
      "company                                                                    \n",
      "1st              6.0   6.666667   8.524475  2.0  3.00   3.5   4.00  24.0   \n",
      "2nd              6.0  15.500000  14.652645  2.0  2.25  13.5  29.25  31.0   \n",
      "\n",
      "        postTestScore                                                       \n",
      "                count       mean        std   min    25%   50%   75%   max  \n",
      "company                                                                     \n",
      "1st               6.0  57.666667  27.485754  25.0  34.25  66.0  70.0  94.0  \n",
      "2nd               6.0  67.000000  14.057027  57.0  58.25  62.0  68.0  94.0  \n",
      "\n",
      "Mean of each company's preTestScore:\n",
      " company\n",
      "1st     6.666667\n",
      "2nd    15.500000\n",
      "Name: preTestScore, dtype: float64\n",
      "\n",
      "Mean preTestScores grouped by regiment and company:\n",
      " regiment    company\n",
      "Dragoons    1st         3.5\n",
      "            2nd        27.5\n",
      "Nighthawks  1st        14.0\n",
      "            2nd        16.5\n",
      "Scouts      1st         2.5\n",
      "            2nd         2.5\n",
      "Name: preTestScore, dtype: float64\n",
      "\n",
      "Mean preTestScores grouped by regiment and company without hierarchical indexing:\n",
      "      regiment company  preTestScore\n",
      "0    Dragoons     1st           3.5\n",
      "1    Dragoons     2nd          27.5\n",
      "2  Nighthawks     1st          14.0\n",
      "3  Nighthawks     2nd          16.5\n",
      "4      Scouts     1st           2.5\n",
      "5      Scouts     2nd           2.5\n",
      "\n",
      "Number of observations in each regiment and company:\n",
      " regiment    company\n",
      "Dragoons    1st        2\n",
      "            2nd        2\n",
      "Nighthawks  1st        2\n",
      "            2nd        2\n",
      "Scouts      1st        2\n",
      "            2nd        2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Create the DataFrame with the given values\n",
    "raw_data = {\n",
    "    'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "    'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd'],\n",
    "    'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'],\n",
    "    'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "    'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]\n",
    "}\n",
    "\n",
    "# Step 3. Assign it to a variable called regiment\n",
    "regiment = pd.DataFrame(raw_data)\n",
    "\n",
    "# Step 4. What is the mean preTestScore from the regiment Nighthawks?\n",
    "mean_preTestScore_Nighthawks = regiment[regiment['regiment'] == 'Nighthawks']['preTestScore'].mean()\n",
    "\n",
    "# Step 5. Present general statistics by company\n",
    "statistics_by_company = regiment.groupby('company').describe()\n",
    "\n",
    "# Step 6. What is the mean of each company's preTestScore?\n",
    "mean_preTestScore_by_company = regiment.groupby('company')['preTestScore'].mean()\n",
    "\n",
    "# Step 7. Present the mean preTestScores grouped by regiment and company\n",
    "mean_preTestScores_grouped = regiment.groupby(['regiment', 'company'])['preTestScore'].mean()\n",
    "\n",
    "# Step 8. Present the mean preTestScores grouped by regiment and company without hierarchical indexing\n",
    "mean_preTestScores_grouped_flat = mean_preTestScores_grouped.reset_index()\n",
    "\n",
    "# Step 9. Group the entire dataframe by regiment and company\n",
    "grouped_dataframe = regiment.groupby(['regiment', 'company'])\n",
    "\n",
    "# Step 10. What is the number of observations in each regiment and company\n",
    "observation_count_per_group = grouped_dataframe.size()\n",
    "\n",
    "# Step 11. Iterate over a group and print the name and the whole data from the regiment\n",
    "for name, group_data in regiment.groupby('regiment'):\n",
    "    print(f\"Regiment: {name}\")\n",
    "    print(group_data)\n",
    "\n",
    "# Displaying the results\n",
    "print(\"\\nMean preTestScore from the regiment Nighthawks:\", mean_preTestScore_Nighthawks)\n",
    "print(\"\\nGeneral statistics by company:\\n\", statistics_by_company)\n",
    "print(\"\\nMean of each company's preTestScore:\\n\", mean_preTestScore_by_company)\n",
    "print(\"\\nMean preTestScores grouped by regiment and company:\\n\", mean_preTestScores_grouped)\n",
    "print(\"\\nMean preTestScores grouped by regiment and company without hierarchical indexing:\\n\", mean_preTestScores_grouped_flat)\n",
    "print(\"\\nNumber of observations in each regiment and company:\\n\", observation_count_per_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b408a",
   "metadata": {},
   "source": [
    "# Ex7 - Student alchohol consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f3511f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last elements of the data set:\n",
      "     school sex  age address famsize Pstatus  Medu  Fedu      Mjob      Fjob  \\\n",
      "390     MS   M   20       U     LE3       A     2     2  Services  Services   \n",
      "391     MS   M   17       U     LE3       T     3     1  Services  Services   \n",
      "392     MS   M   21       R     GT3       T     1     1     Other     Other   \n",
      "393     MS   M   18       R     LE3       T     3     2  Services     Other   \n",
      "394     MS   M   19       U     LE3       T     1     1     Other   At_home   \n",
      "\n",
      "     reason guardian  \n",
      "390  course    other  \n",
      "391  course   mother  \n",
      "392  course    other  \n",
      "393  course   mother  \n",
      "394  course   father  \n",
      "\n",
      "Updated dataframe after multiplying every number by 10:\n",
      "   school sex  age address famsize Pstatus  Medu  Fedu     Mjob      Fjob  ...  \\\n",
      "0     GP   F  180       U     GT3       A    40    40  At_home   Teacher  ...   \n",
      "1     GP   F  170       U     GT3       T    10    10  At_home     Other  ...   \n",
      "2     GP   F  150       U     LE3       T    10    10  At_home     Other  ...   \n",
      "3     GP   F  150       U     GT3       T    40    20   Health  Services  ...   \n",
      "4     GP   F  160       U     GT3       T    30    30    Other     Other  ...   \n",
      "\n",
      "  freetime goout  Dalc  Walc  health absences   G1   G2   G3 legal_drinker  \n",
      "0       30    40    10    10      30       60   50   60   60          True  \n",
      "1       30    30    10    10      30       40   50   50   60         False  \n",
      "2       30    20    20    30      30      100   70   80  100         False  \n",
      "3       20    20    10    10      50       20  150  140  150         False  \n",
      "4       30    20    10    20      50       40   60  100  100         False  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/Students_Alcohol_Consumption/student-mat.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 3. Assign it to a variable called df\n",
    "# Already done in Step 2\n",
    "\n",
    "# Step 4. Slice the dataframe from 'school' until the 'guardian' column\n",
    "sliced_df = df.loc[:, 'school':'guardian']\n",
    "\n",
    "# Step 5. Create a lambda function that will capitalize strings\n",
    "capitalize_string = lambda x: x.capitalize()\n",
    "\n",
    "# Step 6. Capitalize both Mjob and Fjob\n",
    "sliced_df['Mjob'] = sliced_df['Mjob'].apply(capitalize_string)\n",
    "sliced_df['Fjob'] = sliced_df['Fjob'].apply(capitalize_string)\n",
    "\n",
    "# Step 7. Print the last elements of the data set\n",
    "print(\"Last elements of the data set:\\n\", sliced_df.tail())\n",
    "\n",
    "# Step 8. Fix the original dataframe and capitalize Mjob and Fjob\n",
    "df['Mjob'] = df['Mjob'].apply(capitalize_string)\n",
    "df['Fjob'] = df['Fjob'].apply(capitalize_string)\n",
    "\n",
    "# Step 9. Create a function called majority that returns a boolean value to a new column called legal_drinker\n",
    "def majority(age):\n",
    "    return age > 17\n",
    "\n",
    "df['legal_drinker'] = df['age'].apply(majority)\n",
    "\n",
    "# Step 10. Multiply every number of the dataset by 10\n",
    "df_numeric = df.select_dtypes(include='number') * 10\n",
    "df[df_numeric.columns] = df_numeric\n",
    "\n",
    "# Displaying the updated dataframe\n",
    "print(\"\\nUpdated dataframe after multiplying every number by 10:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f5695",
   "metadata": {},
   "source": [
    "# Ex8 - United States - Crime Rates - 1960 - 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5a1d62e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of the columns:\n",
      "Year                  int64\n",
      "Population            int64\n",
      "Total                 int64\n",
      "Violent               int64\n",
      "Property              int64\n",
      "Murder                int64\n",
      "Forcible_Rape         int64\n",
      "Robbery               int64\n",
      "Aggravated_assault    int64\n",
      "Burglary              int64\n",
      "Larceny_Theft         int64\n",
      "Vehicle_Theft         int64\n",
      "dtype: object\n",
      "The most dangerous decade to live in the US was: 2000\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url=\"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/04_Apply/US_Crime_Rates/US_Crime_Rates_1960_2014.csv\"\n",
    "crime = pd.read_csv(url)\n",
    "\n",
    "# Step 3. Assign it to a variable called crime\n",
    "# Already done in Step 2\n",
    "\n",
    "# Step 4. What is the type of the columns?\n",
    "print(\"Type of the columns:\")\n",
    "print(crime.dtypes)\n",
    "\n",
    "# Step 5. Convert the type of the column Year to datetime64\n",
    "crime['Year'] = pd.to_datetime(crime['Year'], format='%Y')\n",
    "\n",
    "# Step 6. Set the Year column as the index of the dataframe\n",
    "crime.set_index('Year', inplace=True)\n",
    "\n",
    "# Step 7. Delete the Total column\n",
    "crime.drop(columns=['Total'], inplace=True)\n",
    "\n",
    "# Step 8. Group the year by decades and sum the values (excluding the Population column)\n",
    "crime_decade = crime.resample('10AS').sum()\n",
    "\n",
    "# Step 9. What is the most dangerous decade to live in the US?\n",
    "most_dangerous_decade = crime_decade.idxmax()[0].year\n",
    "print(\"The most dangerous decade to live in the US was:\", most_dangerous_decade)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf407e9b",
   "metadata": {},
   "source": [
    "# Ex9 - MPG Cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2063e7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in cars1: 198\n",
      "Number of observations in cars2: 200\n",
      "Cars:      mpg  cylinders  displacement horsepower  weight  acceleration  model  \\\n",
      "0    18.0          8           307        130    3504          12.0     70   \n",
      "1    15.0          8           350        165    3693          11.5     70   \n",
      "2    18.0          8           318        150    3436          11.0     70   \n",
      "3    16.0          8           304        150    3433          12.0     70   \n",
      "4    17.0          8           302        140    3449          10.5     70   \n",
      "..    ...        ...           ...        ...     ...           ...    ...   \n",
      "393  27.0          4           140         86    2790          15.6     82   \n",
      "394  44.0          4            97         52    2130          24.6     82   \n",
      "395  32.0          4           135         84    2295          11.6     82   \n",
      "396  28.0          4           120         79    2625          18.6     82   \n",
      "397  31.0          4           119         82    2720          19.4     82   \n",
      "\n",
      "     origin                        car  \n",
      "0         1  chevrolet chevelle malibu  \n",
      "1         1          buick skylark 320  \n",
      "2         1         plymouth satellite  \n",
      "3         1              amc rebel sst  \n",
      "4         1                ford torino  \n",
      "..      ...                        ...  \n",
      "393       1            ford mustang gl  \n",
      "394       2                  vw pickup  \n",
      "395       1              dodge rampage  \n",
      "396       1                ford ranger  \n",
      "397       1                 chevy s-10  \n",
      "\n",
      "[398 rows x 9 columns]\n",
      "Owners :[17732 58567 57613 67416 60891 36243 45403 47103 56993 72043 35757 70026\n",
      " 61884 29935 30430 63600 54512 67620 29650 32089 47230 33983 58095 66374\n",
      " 39152 50665 31921 42469 53984 21921 53804 17163 20072 52619 22877 33430\n",
      " 16871 22599 17496 62954 39675 57968 46921 48523 15797 64811 18219 30246\n",
      " 39999 71608 31321 67489 34129 68887 56504 70839 64866 33676 72495 46230\n",
      " 26723 58890 32591 70324 58136 41752 38605 21021 35006 18560 40332 46055\n",
      " 60444 65793 61522 62647 36368 35737 43647 41681 29627 27134 62883 56390\n",
      " 71402 35571 66811 24781 34340 42257 31298 27372 66403 19420 60830 54876\n",
      " 32455 29324 53019 22012 24396 62080 18918 24359 65836 59259 38482 30127\n",
      " 58727 50725 52237 29165 23752 57565 29505 20795 65624 38647 55133 43254\n",
      " 56216 70153 51530 40392 31221 33819 49402 63682 15973 35848 25215 26157\n",
      " 39777 28824 17418 27843 28242 51223 70319 69971 21521 37626 71894 24123\n",
      " 46454 57947 70890 29254 66939 38700 52073 66762 53299 38310 46785 30041\n",
      " 71356 38306 51950 63171 28160 41635 62874 48075 28392 29368 20302 48920\n",
      " 38334 56235 53062 61634 61451 65360 50260 42112 63717 62390 28592 40213\n",
      " 48538 30875 23286 59258 53763 33728 28640 18091 60663 18912 65586 21594\n",
      " 36752 72524 38532 22997 68006 55800 30620 55899 19845 41763 69268 58606\n",
      " 18533 17937 41221 52707 57512 34608 20021 41773 48902 64177 57448 32340\n",
      " 16913 67086 28429 28907 50489 25088 65080 30588 53395 70417 53214 15469\n",
      " 58295 23717 66015 49488 53040 31975 53953 66474 30912 43086 31591 30115\n",
      " 68414 29447 67573 43665 69401 57870 26052 72560 30741 72368 43739 58986\n",
      " 20103 72091 50050 16740 70270 27579 20846 61078 62805 35165 17775 33987\n",
      " 45752 38563 28928 21356 48930 61774 43651 30997 48391 47232 23962 35435\n",
      " 49009 58671 19403 32582 44844 34381 53429 30939 25787 32191 17121 60353\n",
      " 39451 18886 56690 37450 30212 24987 65975 62881 60485 31583 49215 20181\n",
      " 58349 26042 57914 20208 21177 62749 19863 70332 65944 65635 62394 23389\n",
      " 15894 69526 63208 45142 47904 41813 54041 40968 62395 28688 18419 31552\n",
      " 21004 50620 60232 57521 35803 72191 72852 30139 50954 37302 36533 33161\n",
      " 67513 56106 69545 68469 29676 35989 17562 37597 40484 52420 28173 42389\n",
      " 61402 43271 70387 26605 48191 43310 71006 48735 52138 56820 37571 69355\n",
      " 48201 28097 37621 32813 59889 33930 46200 29299 31713 34996 31363 39785\n",
      " 70913 51736 39534 47319 16040 21250 72508 40182 22704 21772 53079 57179\n",
      " 40907 60192 24144 42741 53779 66950 56120 17195 19680 70751 40687 67237\n",
      " 47058 69865]\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the first dataset cars1 and cars2.\n",
    "cars1 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars1.csv')\n",
    "cars2 = pd.read_csv('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/05_Merge/Auto_MPG/cars2.csv')\n",
    "\n",
    "\n",
    "# Step 4. Oops, it seems our first dataset has some unnamed blank columns, fix cars1\n",
    "cars1 = cars1.loc[:, ~cars1.columns.str.contains('^Unnamed')]\n",
    "\n",
    "# Step 5. What is the number of observations in each dataset?\n",
    "observations_cars1 = cars1.shape[0]\n",
    "observations_cars2 = cars2.shape[0]\n",
    "print(\"Number of observations in cars1:\", observations_cars1)\n",
    "print(\"Number of observations in cars2:\", observations_cars2)\n",
    "\n",
    "# Step 6. Join cars1 and cars2 into a single DataFrame called cars\n",
    "cars = pd.concat([cars1, cars2], ignore_index=True)\n",
    "print(f\"Cars:{cars}\")\n",
    "\n",
    "# Step 7. Oops, there is a column missing, called owners. Create a random number Series from 15,000 to 73,000.\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "owners = np.random.randint(15000, 73001, size=len(cars))\n",
    "print(f\"Owners :{owners}\")\n",
    "\n",
    "# Step 8. Add the column owners to cars\n",
    "cars['owners'] = owners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561adbb",
   "metadata": {},
   "source": [
    "# Ex10 - Fictitious Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8e8a1734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All Data (concatenated along rows):   subject_id first_name last_name\n",
      "0          1       Alex  Anderson\n",
      "1          2        Amy  Ackerman\n",
      "2          3      Allen       Ali\n",
      "3          4      Alice      Aoni\n",
      "4          5     Ayoung   Atiches\n",
      "0          4      Billy    Bonder\n",
      "1          5      Brian     Black\n",
      "2          6       Bran   Balwner\n",
      "3          7      Bryce     Brice\n",
      "4          8      Betty    Btisan\n",
      "\n",
      "All Data Column-wise (concatenated along columns):\n",
      "  subject_id first_name last_name subject_id first_name last_name\n",
      "0          1       Alex  Anderson          4      Billy    Bonder\n",
      "1          2        Amy  Ackerman          5      Brian     Black\n",
      "2          3      Allen       Ali          6       Bran   Balwner\n",
      "3          4      Alice      Aoni          7      Bryce     Brice\n",
      "4          5     Ayoung   Atiches          8      Betty    Btisan\n",
      "\n",
      "Data3 DataFrame:\n",
      "  subject_id  test_id\n",
      "0          1       51\n",
      "1          2       15\n",
      "2          3       15\n",
      "3          4       61\n",
      "4          5       16\n",
      "5          7       14\n",
      "6          8       15\n",
      "7          9        1\n",
      "8         10       61\n",
      "9         11       16\n",
      "\n",
      "Merged Data (all_data and data3 along subject_id):\n",
      "  subject_id first_name last_name  test_id\n",
      "0          1       Alex  Anderson       51\n",
      "1          2        Amy  Ackerman       15\n",
      "2          3      Allen       Ali       15\n",
      "3          4      Alice      Aoni       61\n",
      "4          4      Billy    Bonder       61\n",
      "5          5     Ayoung   Atiches       16\n",
      "6          5      Brian     Black       16\n",
      "7          7      Bryce     Brice       14\n",
      "8          8      Betty    Btisan       15\n",
      "\n",
      "Inner Merge (data1 and data2 on subject_id):\n",
      "  subject_id first_name_x last_name_x first_name_y last_name_y\n",
      "0          4        Alice        Aoni        Billy      Bonder\n",
      "1          5       Ayoung     Atiches        Brian       Black\n",
      "\n",
      "Outer Merge (data1 and data2 on subject_id):\n",
      "  subject_id first_name_x last_name_x first_name_y last_name_y\n",
      "0          1         Alex    Anderson          NaN         NaN\n",
      "1          2          Amy    Ackerman          NaN         NaN\n",
      "2          3        Allen         Ali          NaN         NaN\n",
      "3          4        Alice        Aoni        Billy      Bonder\n",
      "4          5       Ayoung     Atiches        Brian       Black\n",
      "5          6          NaN         NaN         Bran     Balwner\n",
      "6          7          NaN         NaN        Bryce       Brice\n",
      "7          8          NaN         NaN        Betty      Btisan\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Create the 3 DataFrames based on the following raw data\n",
    "raw_data_1 = {\n",
    "    'subject_id': ['1', '2', '3', '4', '5'],\n",
    "    'first_name': ['Alex', 'Amy', 'Allen', 'Alice', 'Ayoung'], \n",
    "    'last_name': ['Anderson', 'Ackerman', 'Ali', 'Aoni', 'Atiches']}\n",
    "data1 = pd.DataFrame(raw_data_1)\n",
    "\n",
    "raw_data_2 = {\n",
    "    'subject_id': ['4', '5', '6', '7', '8'],\n",
    "    'first_name': ['Billy', 'Brian', 'Bran', 'Bryce', 'Betty'], \n",
    "    'last_name': ['Bonder', 'Black', 'Balwner', 'Brice', 'Btisan']}\n",
    "data2 = pd.DataFrame(raw_data_2)\n",
    "\n",
    "raw_data_3 = {\n",
    "    'subject_id': ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11'],\n",
    "    'test_id': [51, 15, 15, 61, 16, 14, 15, 1, 61, 16]}\n",
    "data3 = pd.DataFrame(raw_data_3)\n",
    "\n",
    "# Step 3. Assign each to a variable called data1, data2, data3\n",
    "# Already done in the creation of DataFrames\n",
    "\n",
    "# Step 4. Join the two dataframes along rows and assign all_data\n",
    "all_data = pd.concat([data1, data2])\n",
    "print(\"\\nAll Data (concatenated along rows):\",all_data)\n",
    "\n",
    "# Step 5. Join the two dataframes along columns and assign to all_data_col\n",
    "all_data_col = pd.concat([data1, data2], axis=1)\n",
    "print(\"\\nAll Data Column-wise (concatenated along columns):\")\n",
    "print(all_data_col)\n",
    "\n",
    "# Step 6. Print data3\n",
    "print(\"\\nData3 DataFrame:\")\n",
    "print(data3)\n",
    "\n",
    "# Step 7. Merge all_data and data3 along the subject_id value\n",
    "merged_data = pd.merge(all_data, data3, on='subject_id')\n",
    "print(\"\\nMerged Data (all_data and data3 along subject_id):\")\n",
    "print(merged_data)\n",
    "\n",
    "# Step 8. Merge only the data that has the same 'subject_id' on both data1 and data2\n",
    "merged_data_inner = pd.merge(data1, data2, on='subject_id', how='inner')\n",
    "print(\"\\nInner Merge (data1 and data2 on subject_id):\")\n",
    "print(merged_data_inner)\n",
    "\n",
    "# Step 9. Merge all values in data1 and data2, with matching records from both sides where available.\n",
    "merged_data_outer = pd.merge(data1, data2, on='subject_id', how='outer')\n",
    "print(\"\\nOuter Merge (data1 and data2 on subject_id):\")\n",
    "print(merged_data_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7c623",
   "metadata": {},
   "source": [
    "# Ex11 - Housing Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e4399bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is 'bigcolumn' going only until index 99? False\n",
      "Series 1 (Random numbers from 1 to 4):\n",
      "0     3\n",
      "1     1\n",
      "2     4\n",
      "3     2\n",
      "4     2\n",
      "     ..\n",
      "95    1\n",
      "96    3\n",
      "97    3\n",
      "98    1\n",
      "99    3\n",
      "Length: 100, dtype: int32\n",
      "\n",
      "Series 2 (Random numbers from 1 to 3):\n",
      "0     1\n",
      "1     1\n",
      "2     3\n",
      "3     3\n",
      "4     2\n",
      "     ..\n",
      "95    3\n",
      "96    1\n",
      "97    1\n",
      "98    2\n",
      "99    2\n",
      "Length: 100, dtype: int32\n",
      "\n",
      "Series 3 (Random numbers from 10,000 to 30,000):\n",
      "0     21782\n",
      "1     22900\n",
      "2     19114\n",
      "3     17326\n",
      "4     17557\n",
      "      ...  \n",
      "95    29521\n",
      "96    26151\n",
      "97    11408\n",
      "98    11111\n",
      "99    13877\n",
      "Length: 100, dtype: int32\n",
      "\n",
      "DataFrame:\n",
      "    bedrs  bathrs  price_sqr_meter\n",
      "0       3       1            21782\n",
      "1       1       1            22900\n",
      "2       4       3            19114\n",
      "3       2       3            17326\n",
      "4       2       2            17557\n",
      "..    ...     ...              ...\n",
      "95      1       3            29521\n",
      "96      3       1            26151\n",
      "97      3       1            11408\n",
      "98      1       2            11111\n",
      "99      3       2            13877\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "DataFrame columns renamed to 'bedrs', 'bathrs', 'price_sqr_meter':\n",
      "    bedrs  bathrs  price_sqr_meter\n",
      "0       3       1            21782\n",
      "1       1       1            22900\n",
      "2       4       3            19114\n",
      "3       2       3            17326\n",
      "4       2       2            17557\n",
      "..    ...     ...              ...\n",
      "95      1       3            29521\n",
      "96      3       1            26151\n",
      "97      3       1            11408\n",
      "98      1       2            11111\n",
      "99      3       2            13877\n",
      "\n",
      "[100 rows x 3 columns]\n",
      "\n",
      "One-column DataFrame 'bigcolumn':\n",
      "0          3\n",
      "1          1\n",
      "2          4\n",
      "3          2\n",
      "4          2\n",
      "       ...  \n",
      "295    29521\n",
      "296    26151\n",
      "297    11408\n",
      "298    11111\n",
      "299    13877\n",
      "Length: 300, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 2. Create 3 different Series\n",
    "series1 = pd.Series(np.random.randint(1, 5, size=100))\n",
    "series2 = pd.Series(np.random.randint(1, 4, size=100))\n",
    "series3 = pd.Series(np.random.randint(10000, 30001, size=100))\n",
    "\n",
    "# Step 3. Create a DataFrame by joining the Series by column\n",
    "df = pd.DataFrame({'bedrs': series1, 'bathrs': series2, 'price_sqr_meter': series3})\n",
    "\n",
    "# Step 4. Change the name of the columns\n",
    "df.columns = ['bedrs', 'bathrs', 'price_sqr_meter']\n",
    "\n",
    "# Step 5. Create a one-column DataFrame with the values of the 3 Series\n",
    "bigcolumn = pd.concat([series1, series2, series3], axis=0)\n",
    "\n",
    "# Step 6. Check if 'bigcolumn' goes only until index 99\n",
    "print(\"Is 'bigcolumn' going only until index 99?\", len(bigcolumn) == 100)\n",
    "\n",
    "# Step 7. Reindex the DataFrame so it goes from 0 to 299\n",
    "bigcolumn.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print all the objects with proper messages\n",
    "print(\"Series 1 (Random numbers from 1 to 4):\")\n",
    "print(series1)\n",
    "\n",
    "print(\"\\nSeries 2 (Random numbers from 1 to 3):\")\n",
    "print(series2)\n",
    "\n",
    "print(\"\\nSeries 3 (Random numbers from 10,000 to 30,000):\")\n",
    "print(series3)\n",
    "\n",
    "print(\"\\nDataFrame:\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nDataFrame columns renamed to 'bedrs', 'bathrs', 'price_sqr_meter':\")\n",
    "print(df)\n",
    "\n",
    "print(\"\\nOne-column DataFrame 'bigcolumn':\")\n",
    "print(bigcolumn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dc8233",
   "metadata": {},
   "source": [
    "# Ex12 - US - Baby Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "affdaf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries of the dataset:\n",
      "   Unnamed: 0     Id      Name  Year Gender State  Count\n",
      "0       11349  11350      Emma  2004      F    AK     62\n",
      "1       11350  11351   Madison  2004      F    AK     48\n",
      "2       11351  11352    Hannah  2004      F    AK     46\n",
      "3       11352  11353     Grace  2004      F    AK     44\n",
      "4       11353  11354     Emily  2004      F    AK     41\n",
      "5       11354  11355   Abigail  2004      F    AK     37\n",
      "6       11355  11356    Olivia  2004      F    AK     33\n",
      "7       11356  11357  Isabella  2004      F    AK     30\n",
      "8       11357  11358    Alyssa  2004      F    AK     29\n",
      "9       11358  11359    Sophia  2004      F    AK     28\n",
      "\n",
      "More male or female names in the dataset: F\n",
      "\n",
      "Number of different names in the dataset: 17632\n",
      "\n",
      "Name with the most occurrences: Riley\n",
      "\n",
      "Number of different names with the least occurrences: 3682\n",
      "\n",
      "Median name occurrence: 8.0\n",
      "\n",
      "Standard deviation of name occurrences: 122.02996350814125\n",
      "\n",
      "Summary of name occurrences:\n",
      "count    17632.000000\n",
      "mean        57.644907\n",
      "std        122.029964\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          8.000000\n",
      "75%         39.000000\n",
      "max       1112.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/06_Stats/US_Baby_Names/US_Baby_Names_right.csv\"\n",
    "baby_names = pd.read_csv(url)\n",
    "\n",
    "# Step 3. Assign the dataset to a variable called baby_names\n",
    "baby_names = pd.read_csv(url)\n",
    "\n",
    "# Step 4. See the first 10 entries\n",
    "print(\"First 10 entries of the dataset:\")\n",
    "print(baby_names.head(10))\n",
    "\n",
    "# Step 5. Delete the column 'Unnamed: 0' and 'Id'\n",
    "baby_names.drop(['Unnamed: 0', 'Id'], axis=1, inplace=True)\n",
    "\n",
    "# Step 6. Check if there are more male or female names in the dataset\n",
    "gender_counts = baby_names['Gender'].value_counts()\n",
    "print(\"\\nMore male or female names in the dataset:\", gender_counts.idxmax())\n",
    "\n",
    "# Step 7. Group the dataset by name and assign to names\n",
    "names = baby_names.groupby('Name')\n",
    "\n",
    "# Step 8. Count the number of different names in the dataset\n",
    "num_different_names = len(names)\n",
    "print(\"\\nNumber of different names in the dataset:\", num_different_names)\n",
    "\n",
    "# Step 9. Find the name with the most occurrences\n",
    "most_occurrences_name = names.size().idxmax()\n",
    "print(\"\\nName with the most occurrences:\", most_occurrences_name)\n",
    "\n",
    "# Step 10. Find the number of different names with the least occurrences\n",
    "least_occurrences = names.size().min()\n",
    "num_least_occurrences_names = sum(names.size() == least_occurrences)\n",
    "print(\"\\nNumber of different names with the least occurrences:\", num_least_occurrences_names)\n",
    "\n",
    "# Step 11. Find the median name occurrence\n",
    "median_occurrences = names.size().median()\n",
    "print(\"\\nMedian name occurrence:\", median_occurrences)\n",
    "\n",
    "# Step 12. Find the standard deviation of name occurrences\n",
    "std_occurrences = names.size().std()\n",
    "print(\"\\nStandard deviation of name occurrences:\", std_occurrences)\n",
    "\n",
    "# Step 13. Get a summary with the mean, min, max, std, and quartiles of name occurrences\n",
    "summary = names.size().describe()\n",
    "print(\"\\nSummary of name occurrences:\")\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b7751",
   "metadata": {},
   "source": [
    "# Ex13 - Wind Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f6047d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13348\\1387773874.py:9: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  data['Date'] = pd.to_datetime(data['Yr'].astype(str) + '-' + data['Mo'].astype(str) + '-' + data['Dy'].astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values for each location:\n",
      "RPT    6\n",
      "VAL    3\n",
      "ROS    2\n",
      "KIL    5\n",
      "SHA    2\n",
      "BIR    0\n",
      "DUB    3\n",
      "CLA    2\n",
      "MUL    3\n",
      "CLO    1\n",
      "BEL    0\n",
      "MAL    4\n",
      "dtype: int64\n",
      "\n",
      "Total non-missing values: 78857\n",
      "\n",
      "Mean windspeed: 10.227982360836938\n",
      "\n",
      "Location statistics:\n",
      "      Min    Max       Mean       Std\n",
      "RPT  0.67  35.80  12.362987  5.618413\n",
      "VAL  0.21  33.37  10.644314  5.267356\n",
      "ROS  1.50  33.84  11.660526  5.008450\n",
      "KIL  0.00  28.46   6.306468  3.605811\n",
      "SHA  0.13  37.54  10.455834  4.936125\n",
      "BIR  0.00  26.16   7.092254  3.968683\n",
      "DUB  0.00  30.37   9.797343  4.977555\n",
      "CLA  0.00  31.08   8.495053  4.499449\n",
      "MUL  0.00  25.88   8.493590  4.166872\n",
      "CLO  0.04  28.21   8.707332  4.503954\n",
      "BEL  0.13  42.38  13.121007  5.835037\n",
      "MAL  0.67  42.54  15.599079  6.699794\n",
      "\n",
      "Day statistics:\n",
      "             Min    Max       Mean       Std\n",
      "Date                                        \n",
      "1961-01-01  9.29  18.50  13.018182  2.808875\n",
      "1961-01-02  6.50  17.54  11.336364  3.188994\n",
      "1961-01-03  6.17  18.50  11.641818  3.681912\n",
      "1961-01-04  1.79  11.75   6.619167  3.198126\n",
      "1961-01-05  6.17  13.33  10.630000  2.445356\n",
      "...          ...    ...        ...       ...\n",
      "1978-12-27  8.08  40.08  16.708333  7.868076\n",
      "1978-12-28  5.00  41.46  15.150000  9.687857\n",
      "1978-12-29  8.71  29.58  14.890000  5.756836\n",
      "1978-12-30  9.13  28.79  15.367500  5.540437\n",
      "1978-12-31  9.59  27.29  15.402500  5.702483\n",
      "\n",
      "[6574 rows x 4 columns]\n",
      "\n",
      "Average windspeed in January for each location:\n",
      "RPT    14.847325\n",
      "VAL    12.914560\n",
      "ROS    13.299624\n",
      "KIL     7.199498\n",
      "SHA    11.667734\n",
      "BIR     8.054839\n",
      "DUB    11.819355\n",
      "CLA     9.512047\n",
      "MUL     9.543208\n",
      "CLO    10.053566\n",
      "BEL    14.550520\n",
      "MAL    18.028763\n",
      "dtype: float64\n",
      "\n",
      "Yearly frequency for each location:\n",
      "                  RPT        VAL        ROS       KIL        SHA       BIR  \\\n",
      "Date                                                                         \n",
      "1961-12-31  12.299583  10.351796  11.362369  6.958227  10.881763  7.729726   \n",
      "1962-12-31  12.246923  10.110438  11.732712  6.960440  10.657918  7.393068   \n",
      "1963-12-31  12.813452  10.836986  12.541151  7.330055  11.724110  8.434712   \n",
      "1964-12-31  12.363661  10.920164  12.104372  6.787787  11.454481  7.570874   \n",
      "1965-12-31  12.451370  11.075534  11.848767  6.858466  11.024795  7.478110   \n",
      "1966-12-31  13.461973  11.557205  12.020630  7.345726  11.805041  7.793671   \n",
      "1967-12-31  12.737151  10.990986  11.739397  7.143425  11.630740  7.368164   \n",
      "1968-12-31  11.835628  10.468197  11.409754  6.477678  10.760765  6.067322   \n",
      "1969-12-31  11.166356   9.723699  10.902000  5.767973   9.873918  6.189973   \n",
      "1970-12-31  12.600329  10.726932  11.730247  6.217178  10.567370  7.609452   \n",
      "1971-12-31  11.273123   9.095178  11.088329  5.241507   9.440329  6.097151   \n",
      "1972-12-31  12.463962  10.561311  12.058333  5.929699   9.430410  6.358825   \n",
      "1973-12-31  11.828466  10.680493  10.680493  5.547863   9.640877  6.548740   \n",
      "1974-12-31  13.643096  11.811781  12.336356  6.427041  11.110986  6.809781   \n",
      "1975-12-31  12.008575  10.293836  11.564712  5.269096   9.190082  5.668521   \n",
      "1976-12-31  11.737842  10.203115  10.761230  5.109426   8.846339  6.311038   \n",
      "1977-12-31  13.099616  11.144493  12.627836  6.073945  10.003836  8.586438   \n",
      "1978-12-31  12.504356  11.044274  11.380000  6.082356  10.167233  7.650658   \n",
      "\n",
      "                  DUB        CLA       MUL        CLO        BEL        MAL  \n",
      "Date                                                                         \n",
      "1961-12-31   9.733923   8.858788  8.647652   9.835577  13.502795  13.680773  \n",
      "1962-12-31  11.020712   8.793753  8.316822   9.676247  12.930685  14.323956  \n",
      "1963-12-31  11.075699  10.336548  8.903589  10.224438  13.638877  14.999014  \n",
      "1964-12-31  10.259153   9.467350  7.789016  10.207951  13.740546  14.910301  \n",
      "1965-12-31  10.618712   8.879918  7.907425   9.918082  12.964247  15.591644  \n",
      "1966-12-31  10.579808   8.835096  8.514438   9.768959  14.265836  16.307260  \n",
      "1967-12-31  10.652027   9.325616  8.645014   9.547425  14.774548  17.135945  \n",
      "1968-12-31   8.859180   8.255519  7.224945   7.832978  12.808634  15.017486  \n",
      "1969-12-31   8.564493   7.711397  7.924521   7.754384  12.621233  15.762904  \n",
      "1970-12-31   9.609890   8.334630  9.297616   8.289808  13.183644  16.456027  \n",
      "1971-12-31   8.385890   6.757315  7.915370   7.229753  12.208932  15.025233  \n",
      "1972-12-31   9.704508   7.680792  8.357295   7.515273  12.727377  15.028716  \n",
      "1973-12-31   8.482110   7.614274  8.245534   7.812411  12.169699  15.441096  \n",
      "1974-12-31  10.084603   9.896986  9.331753   8.736356  13.252959  16.947671  \n",
      "1975-12-31   8.562603   7.843836  8.797945   7.382822  12.631671  15.307863  \n",
      "1976-12-31   9.149126   7.146202  8.883716   7.883087  12.332377  15.471448  \n",
      "1977-12-31  11.523205   8.378384  9.098192   8.821616  13.459068  16.590849  \n",
      "1978-12-31   9.489342   8.800466  9.089753   8.301699  12.967397  16.771370  \n",
      "\n",
      "Monthly frequency for each location:\n",
      "                  RPT        VAL        ROS       KIL        SHA        BIR  \\\n",
      "Date                                                                          \n",
      "1961-01-31  14.841333  11.988333  13.431613  7.736774  11.072759   8.588065   \n",
      "1961-02-28  16.269286  14.975357  14.441481  9.230741  13.852143  10.937500   \n",
      "1961-03-31  10.890000  11.296452  10.752903  7.284000  10.509355   8.866774   \n",
      "1961-04-30  10.722667   9.427667   9.998000  5.830667   8.435000   6.495000   \n",
      "1961-05-31   9.860968   8.850000  10.818065  5.905333   9.490323   6.574839   \n",
      "...               ...        ...        ...       ...        ...        ...   \n",
      "1978-08-31   9.645161   8.259355   9.032258  4.502903   7.368065   5.935161   \n",
      "1978-09-30  10.913667  10.895000  10.635000  5.725000  10.372000   9.278333   \n",
      "1978-10-31   9.897742   8.670968   9.295806  4.721290   8.525161   6.774194   \n",
      "1978-11-30  16.151667  14.802667  13.508000  7.317333  11.475000   8.743000   \n",
      "1978-12-31  16.175484  13.748065  15.635161  7.094839  11.398710   9.241613   \n",
      "\n",
      "                  DUB        CLA        MUL        CLO        BEL        MAL  \n",
      "Date                                                                          \n",
      "1961-01-31  11.184839   9.245333   9.085806  10.107419  13.880968  14.703226  \n",
      "1961-02-28  11.890714  11.846071  11.821429  12.714286  18.583214  15.411786  \n",
      "1961-03-31   9.644194   9.829677  10.294138  11.251935  16.410968  15.720000  \n",
      "1961-04-30   6.925333   7.094667   7.342333   7.237000  11.147333  10.278333  \n",
      "1961-05-31   7.604000   8.177097   8.039355   8.499355  11.900323  12.011613  \n",
      "...               ...        ...        ...        ...        ...        ...  \n",
      "1978-08-31   5.650323   5.417742   7.241290   5.536774  10.466774  12.054194  \n",
      "1978-09-30  10.790333   9.583000  10.069333   8.939000  15.680333  19.391333  \n",
      "1978-10-31   8.115484   7.337742   8.297742   8.243871  13.776774  17.150000  \n",
      "1978-11-30  11.492333   9.657333  10.701333  10.676000  17.404667  20.723000  \n",
      "1978-12-31  12.077419  10.194839  10.616774  11.028710  13.859677  21.371613  \n",
      "\n",
      "[216 rows x 12 columns]\n",
      "\n",
      "Weekly frequency for each location:\n",
      "                  RPT        VAL        ROS        KIL        SHA        BIR  \\\n",
      "Date                                                                           \n",
      "1961-01-01  15.040000  14.960000  13.170000   9.290000        NaN   9.870000   \n",
      "1961-01-08  13.541429  11.486667  10.487143   6.417143   9.474286   6.435714   \n",
      "1961-01-15  12.468571   8.967143  11.958571   4.630000   7.351429   5.072857   \n",
      "1961-01-22  13.204286   9.862857  12.982857   6.328571   8.966667   7.417143   \n",
      "1961-01-29  19.880000  16.141429  18.225714  12.720000  17.432857  14.828571   \n",
      "...               ...        ...        ...        ...        ...        ...   \n",
      "1978-12-03  14.934286  11.232857  13.941429   5.565714  10.215714   8.618571   \n",
      "1978-12-10  20.740000  19.190000  17.034286   9.777143  15.287143  12.774286   \n",
      "1978-12-17  16.758571  14.692857  14.987143   6.917143  11.397143   7.272857   \n",
      "1978-12-24  11.155714   8.008571  13.172857   4.004286   7.825714   6.290000   \n",
      "1978-12-31  14.951429  11.801429  16.035714   6.507143   9.660000   8.620000   \n",
      "\n",
      "                  DUB        CLA        MUL        CLO        BEL        MAL  \n",
      "Date                                                                          \n",
      "1961-01-01  13.670000  10.250000  10.830000  12.580000  18.500000  15.040000  \n",
      "1961-01-08  11.061429   6.616667   8.434286   8.497143  12.481429  13.238571  \n",
      "1961-01-15   7.535714   6.820000   5.712857   7.571429  11.125714  11.024286  \n",
      "1961-01-22   9.257143   7.875714   7.145714   8.124286   9.821429  11.434286  \n",
      "1961-01-29  15.528571  15.160000  14.480000  15.640000  20.930000  22.530000  \n",
      "...               ...        ...        ...        ...        ...        ...  \n",
      "1978-12-03   9.642857   7.685714   9.011429   9.547143  11.835714  18.728571  \n",
      "1978-12-10  14.437143  12.488571  13.870000  14.082857  18.517143  23.061429  \n",
      "1978-12-17  10.208571   7.967143   9.168571   8.565714  11.102857  15.562857  \n",
      "1978-12-24   7.798571   8.667143   7.151429   8.072857  11.845714  18.977143  \n",
      "1978-12-31  13.708571  10.477143  10.868571  11.471429  12.947143  26.844286  \n",
      "\n",
      "[940 rows x 12 columns]\n",
      "\n",
      "Weekly statistics for the first 52 weeks:\n",
      "              RPT                                VAL                    \\\n",
      "              min    max       mean       std    min    max       mean   \n",
      "Date                                                                     \n",
      "1961-01-01  15.04  15.04  15.040000       NaN  14.96  14.96  14.960000   \n",
      "1961-01-08  10.58  18.50  13.541429  2.631321   6.63  16.88  11.486667   \n",
      "1961-01-15   9.04  19.75  12.468571  3.555392   3.54  12.08   8.967143   \n",
      "1961-01-22   4.92  19.83  13.204286  5.337402   3.42  14.37   9.862857   \n",
      "1961-01-29  13.62  25.04  19.880000  4.619061   9.96  23.91  16.141429   \n",
      "1961-02-05  10.58  24.21  16.827143  5.251408   9.46  24.21  15.460000   \n",
      "1961-02-12  16.00  24.54  19.684286  3.587677  11.54  21.42  16.417143   \n",
      "1961-02-19   6.04  22.50  15.130000  5.064609  11.63  20.17  15.091429   \n",
      "1961-02-26   7.79  25.80  15.221429  7.020716   7.08  21.50  13.625714   \n",
      "1961-03-05  10.96  13.33  12.101429  0.997721   8.83  17.00  12.951429   \n",
      "1961-03-12   4.88  14.79   9.376667  3.732263   8.08  16.96  11.578571   \n",
      "1961-03-19   4.92  16.88  11.911429  3.860036   9.46  15.54  13.501429   \n",
      "1961-03-26   6.29  15.00   9.567143  3.613298   2.58  11.63   8.387143   \n",
      "1961-04-02   5.88  18.25  10.757143  5.046922   3.50  16.29   8.852857   \n",
      "1961-04-09   4.50  18.12  11.964286  4.604392   7.04  14.62  10.654286   \n",
      "1961-04-16   4.71  15.50   8.965714  3.937727   4.83  12.25   8.000000   \n",
      "1961-04-23   4.00  21.09  12.621429  5.676655   3.71  15.41  10.438571   \n",
      "1961-04-30   4.08  16.29  10.117143  4.349662   6.50  14.46   9.798571   \n",
      "1961-05-07   9.87  23.00  15.367143  5.025507  10.29  19.79  13.970000   \n",
      "1961-05-14   3.54  12.79   7.772857  3.371022   3.96  15.12   8.712857   \n",
      "1961-05-21   4.88  15.04   8.225714  3.631730   3.58  10.17   5.631667   \n",
      "1961-05-28   4.96  11.79   8.155714  2.739433   3.67  12.50   7.388571   \n",
      "1961-06-04   7.00  15.92  10.321429  3.099701   4.75   9.79   7.407143   \n",
      "1961-06-11   8.29  14.42  10.917143  2.248597   6.83  11.54   8.992857   \n",
      "1961-06-18   6.13  14.33  10.571429  3.009482   4.12  14.54   9.565714   \n",
      "1961-06-25   4.00   9.00   7.345714  1.982035   3.25   9.50   6.108571   \n",
      "1961-07-02   7.21  13.13  10.236667  2.557856   6.34  14.37   9.482857   \n",
      "1961-07-09   7.29  17.50  11.715714  3.664855   3.75  10.75   7.220000   \n",
      "1961-07-16   8.63  22.50  16.680000  5.168710   7.87  19.29  13.518571   \n",
      "1961-07-23   3.04   5.88   4.202857  1.047978   2.92   6.79   4.255714   \n",
      "1961-07-30   6.13  16.08  10.561429  4.157641   4.63  13.79   8.445714   \n",
      "1961-08-06   7.67  16.08  10.870000  2.950887   5.09  15.79   8.792857   \n",
      "1961-08-13   2.88  14.21  10.058333  4.422268   4.42  10.00   7.941429   \n",
      "1961-08-20  13.13  18.91  15.607143  2.283635   7.25  13.62  10.565714   \n",
      "1961-08-27   7.67  18.16  12.391429  3.395857   6.87  14.58  11.430000   \n",
      "1961-09-03   3.63  14.46   8.678571  4.398615   1.13  22.00   8.821429   \n",
      "1961-09-10   5.00  17.62  10.541429  5.207278   3.04  13.59   8.798571   \n",
      "1961-09-17   5.33  28.75  17.160000  7.679190   7.12  22.08  14.440000   \n",
      "1961-09-24   6.92  10.25   8.500000  1.267399   2.92  13.62   7.154286   \n",
      "1961-10-01  10.34  23.21  16.044286  4.559572   6.96  13.62  11.361667   \n",
      "1961-10-08   3.13  16.08  11.250000  5.596710   3.63  16.96   8.757143   \n",
      "1961-10-15   3.71  17.12  10.881667  4.780675   6.13  13.25   9.734286   \n",
      "1961-10-22  10.46  28.62  19.260000  7.888314   3.75  19.46  13.364286   \n",
      "1961-10-29   4.33  26.42  16.077143  7.957637   2.75  21.25  11.867143   \n",
      "1961-11-05   5.88  15.79  11.571429  3.369201   3.96  13.46   9.590000   \n",
      "1961-11-12   5.46  16.08  10.428571  3.939811   5.50  10.75   7.690000   \n",
      "1961-11-19   7.50  15.00  10.798571  2.784358   4.21  13.00   7.951429   \n",
      "1961-11-26   5.75  14.29   9.154286  3.214368   3.29  11.79   7.337143   \n",
      "1961-12-03   7.92  23.75  12.608571  5.704669   4.67  18.71  10.442857   \n",
      "1961-12-10  10.83  23.71  17.362857  4.890152   9.21  21.37  14.362857   \n",
      "1961-12-17   9.29  21.34  14.985714  4.095106   8.63  17.62  13.472857   \n",
      "1961-12-24  11.46  24.41  15.757143  4.959717   9.79  16.13  11.971429   \n",
      "\n",
      "                        ROS         ...        CLO              BEL         \\\n",
      "                 std    min    max  ...       mean       std    min    max   \n",
      "Date                                ...                                      \n",
      "1961-01-01       NaN  13.17  13.17  ...  12.580000       NaN  18.50  18.50   \n",
      "1961-01-08  3.949525   7.62  12.33  ...   8.497143  1.704941   5.46  17.54   \n",
      "1961-01-15  3.148945   7.08  19.50  ...   7.571429  4.084293   5.25  20.71   \n",
      "1961-01-22  3.837785   7.29  20.79  ...   8.124286  4.783952   6.50  15.92   \n",
      "1961-01-29  5.170224  12.67  25.84  ...  15.640000  3.713368  14.04  27.71   \n",
      "1961-02-05  5.187395   9.04  19.70  ...   9.460000  2.839501   9.17  19.33   \n",
      "1961-02-12  3.608373  13.67  21.34  ...  14.440000  1.746749  15.21  26.38   \n",
      "1961-02-19  3.575012   6.13  19.41  ...  13.542857  2.531361  14.09  29.63   \n",
      "1961-02-26  5.147348   6.08  22.42  ...  12.730000  4.920064   9.59  23.21   \n",
      "1961-03-05  2.851955   8.17  13.67  ...  12.370000  1.593685  11.58  23.45   \n",
      "1961-03-12  3.230167   7.54  16.38  ...  10.458571  3.655113  10.21  22.71   \n",
      "1961-03-19  2.352867   5.25  13.96  ...  11.627143  3.099472  11.29  22.79   \n",
      "1961-03-26  3.657265   4.79  15.63  ...  11.481429  2.538224   8.25  21.34   \n",
      "1961-04-02  4.687315   5.09  14.96  ...   9.631429  3.191115   7.21  18.63   \n",
      "1961-04-09  2.845399   9.29  18.29  ...   7.238571  2.336182   7.62  17.16   \n",
      "1961-04-16  2.607118   3.92  15.79  ...   6.178571  2.161137   5.75  16.17   \n",
      "1961-04-23  4.631736   3.33  17.00  ...   9.551429  3.347972   6.75  19.21   \n",
      "1961-04-30  2.871425   2.54  14.96  ...   6.124286  2.840568   5.13  13.04   \n",
      "1961-05-07  3.750835   8.42  21.21  ...  11.585714  3.620819   4.79  28.08   \n",
      "1961-05-14  3.782947   4.63  12.33  ...   7.822857  5.460237   6.54  18.66   \n",
      "1961-05-21  2.468906   5.91  15.96  ...   7.114286  2.216889   6.63  12.00   \n",
      "1961-05-28  3.378537   3.58  20.96  ...   7.535714  2.575661   6.13  14.33   \n",
      "1961-06-04  1.868125   6.83  15.96  ...   9.035714  2.096989   8.71  17.00   \n",
      "1961-06-11  1.524836   6.04  11.58  ...   8.397143  2.158323   5.37  16.17   \n",
      "1961-06-18  3.509444   6.50  15.50  ...   9.594286  3.792400   7.29  25.25   \n",
      "1961-06-25  2.212460   5.13  10.37  ...  11.257143  2.286218  11.00  19.08   \n",
      "1961-07-02  2.902411   7.62  10.79  ...  10.268571  1.564144   6.96  21.87   \n",
      "1961-07-09  2.686658   7.41  14.92  ...  10.547143  3.657179   7.08  20.41   \n",
      "1961-07-16  3.849630   8.00  14.29  ...  10.157143  3.271899   5.04  20.25   \n",
      "1961-07-23  1.400010   4.08  12.67  ...   6.041429  1.439785   4.21  10.13   \n",
      "1961-07-30  3.203206   6.42  15.67  ...   9.470000  4.350268   7.87  22.17   \n",
      "1961-08-06  3.985226   5.54  15.59  ...   8.951429  2.903018   6.17  18.54   \n",
      "1961-08-13  2.053326   6.00  11.63  ...   7.235000  2.073777   5.88  14.29   \n",
      "1961-08-20  2.523416   9.17  14.04  ...  12.244286  2.730237   9.59  21.92   \n",
      "1961-08-27  3.174702   9.50  17.83  ...  12.507143  3.855302   5.75  23.38   \n",
      "1961-09-03  7.474025   2.42  12.75  ...   8.924286  3.993736   4.79  24.71   \n",
      "1961-09-10  4.003996   3.63  14.88  ...   7.565714  3.649278   5.41  15.83   \n",
      "1961-09-17  5.360585   7.29  26.50  ...  14.268571  5.128338  13.92  23.91   \n",
      "1961-09-24  3.445262   5.33  12.17  ...   6.798571  2.354092   4.54  20.96   \n",
      "1961-10-01  2.812482   7.87  17.58  ...  11.840000  3.908397   7.79  21.37   \n",
      "1961-10-08  5.060803   4.96  17.83  ...   7.662857  4.296870   4.79  17.83   \n",
      "1961-10-15  2.707483   4.21  20.96  ...   9.494286  3.569308   8.54  20.46   \n",
      "1961-10-22  5.998199   9.92  22.63  ...  16.461429  5.890511  10.88  24.71   \n",
      "1961-10-29  6.879973   4.79  23.09  ...  12.952857  6.277629   8.50  27.29   \n",
      "1961-11-05  3.900278   6.54  14.33  ...  11.864286  2.784450  10.29  20.17   \n",
      "1961-11-12  2.141191   3.83  20.41  ...   6.415714  2.843518   6.13  12.58   \n",
      "1961-11-19  3.208548   6.46  32.71  ...   6.031429  4.402588   0.71   9.59   \n",
      "1961-11-26  3.277904   4.42  14.46  ...   9.582857  2.747452   6.50  20.46   \n",
      "1961-12-03  5.107089   8.00  19.92  ...   7.911429  3.680477   5.50  19.04   \n",
      "1961-12-10  4.115506   9.71  20.54  ...  12.022857  4.156207  10.71  21.79   \n",
      "1961-12-17  3.587886   9.04  19.00  ...  11.058571  4.633398   5.50  22.83   \n",
      "1961-12-24  2.220866   8.08  22.13  ...   7.697143  4.637096   5.29  17.67   \n",
      "\n",
      "                                   MAL                              \n",
      "                 mean       std    min    max       mean       std  \n",
      "Date                                                                \n",
      "1961-01-01  18.500000       NaN  15.04  15.04  15.040000       NaN  \n",
      "1961-01-08  12.481429  4.349139  10.88  16.46  13.238571  1.773062  \n",
      "1961-01-15  11.125714  5.552215   5.17  16.92  11.024286  4.692355  \n",
      "1961-01-22   9.821429  3.626584   6.79  17.96  11.434286  4.237239  \n",
      "1961-01-29  20.930000  5.210726  17.50  27.63  22.530000  3.874721  \n",
      "1961-02-05  14.012857  4.210858   7.17  19.25  11.935714  4.336104  \n",
      "1961-02-12  21.832857  4.063753  17.04  21.84  19.155714  1.828705  \n",
      "1961-02-19  21.167143  5.910938  10.96  22.58  16.584286  4.685377  \n",
      "1961-02-26  16.304286  5.091162   6.67  23.87  14.322857  6.182283  \n",
      "1961-03-05  17.842857  4.332331   8.83  17.54  13.951667  3.021387  \n",
      "1961-03-12  16.701429  4.358759   5.54  22.54  14.420000  5.769890  \n",
      "1961-03-19  19.350000  3.779727  11.34  22.95  16.227143  4.331958  \n",
      "1961-03-26  14.037143  4.318069  13.13  22.50  18.134286  3.701846  \n",
      "1961-04-02  13.471429  4.179854   7.17  19.58  13.900000  3.924555  \n",
      "1961-04-09  11.712857  3.147781   7.21  15.34  11.371429  2.598271  \n",
      "1961-04-16   9.482857  3.641464   5.66  12.87   8.690000  2.747842  \n",
      "1961-04-23  13.620000  4.735096   4.96  20.46  12.470000  5.908542  \n",
      "1961-04-30   9.720000  2.948237   2.67  17.50   8.637143  5.108365  \n",
      "1961-05-07  17.548571  8.003490   3.83  26.58  14.571429  7.728504  \n",
      "1961-05-14  10.421429  3.968272   3.33  26.30  10.382857  7.858246  \n",
      "1961-05-21   9.624286  1.975853   5.91  14.96  10.612857  3.310819  \n",
      "1961-05-28  10.518571  3.024524   8.00  17.04  11.697143  3.811818  \n",
      "1961-06-04  12.298571  2.611139  10.63  17.96  13.597143  2.593586  \n",
      "1961-06-11  10.148571  3.993062   5.96  19.83  12.250000  4.925055  \n",
      "1961-06-18  15.351429  6.477887   6.13  24.71  15.025714  6.242673  \n",
      "1961-06-25  14.370000  2.498386  13.75  21.50  17.410000  3.063011  \n",
      "1961-07-02  14.535714  6.303747   8.50  16.79  12.133333  3.652313  \n",
      "1961-07-09  12.220000  4.537988  12.08  21.29  15.987143  3.665705  \n",
      "1961-07-16  13.520000  4.971060   5.96  21.96  12.524286  4.974273  \n",
      "1961-07-23   7.524286  2.050218   5.41  10.92   8.415714  2.133994  \n",
      "1961-07-30  12.841429  5.580903   6.13  25.37  13.761429  6.664574  \n",
      "1961-08-06  11.595714  4.901377   9.08  20.25  13.760000  4.448251  \n",
      "1961-08-13  10.934286  2.931302   5.88  15.16  10.125714  3.356585  \n",
      "1961-08-20  14.922857  4.086725  13.04  24.30  16.626667  3.934238  \n",
      "1961-08-27  16.251429  6.711322   8.29  22.29  16.485714  4.947608  \n",
      "1961-09-03  13.664286  7.678051   5.41  22.54  11.022857  6.308087  \n",
      "1961-09-10  10.700000  4.220584   3.37  20.25  11.034286  6.049619  \n",
      "1961-09-17  19.878571  4.464252  14.67  33.09  18.984286  6.332885  \n",
      "1961-09-24  11.018571  5.235868   5.25  14.62   9.814286  3.113507  \n",
      "1961-10-01  16.208571  5.091268   4.04  17.16  13.338571  4.696504  \n",
      "1961-10-08   8.810000  4.800403   4.83  19.62  11.410000  5.462002  \n",
      "1961-10-15  14.451429  4.113200  10.75  21.04  15.260000  4.098130  \n",
      "1961-10-22  17.477143  5.645871  13.46  33.45  23.641429  7.468377  \n",
      "1961-10-29  15.592857  7.056150   9.83  30.88  18.404286  8.340881  \n",
      "1961-11-05  16.322857  4.038493  13.37  23.58  19.195714  3.870800  \n",
      "1961-11-12   9.208571  2.532196   5.71  15.54  10.858571  3.690752  \n",
      "1961-11-19   5.875714  3.643285   2.00  13.25   5.737143  3.787654  \n",
      "1961-11-26  11.772857  5.407223   4.25  22.58  12.732857  6.475867  \n",
      "1961-12-03  11.464286  5.552648   5.88  21.29  14.725714  5.233192  \n",
      "1961-12-10  15.975714  4.667933   7.58  29.33  16.241429  7.345893  \n",
      "1961-12-17  15.112857  6.531043   6.50  21.12  14.644286  5.665006  \n",
      "1961-12-24   9.958571  5.065308   2.62  16.62   8.164286  5.048035  \n",
      "\n",
      "[52 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1. Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2. Import the dataset from the provided address\n",
    "url = \"https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/06_Stats/Wind_Stats/wind.data\"\n",
    "data = pd.read_csv(url, delim_whitespace=True)\n",
    "\n",
    "# Step 3. Replace the first 3 columns by a proper datetime index\n",
    "data['Date'] = pd.to_datetime(data['Yr'].astype(str) + '-' + data['Mo'].astype(str) + '-' + data['Dy'].astype(str))\n",
    "data.set_index('Date', inplace=True)\n",
    "data.drop(['Yr', 'Mo', 'Dy'], axis=1, inplace=True)\n",
    "\n",
    "# Step 4. Define a function to fix the year\n",
    "def fix_year(year):\n",
    "    if year > 2000:\n",
    "        year -= 100\n",
    "    return year\n",
    "\n",
    "data.index = data.index.map(lambda x: x.replace(year=fix_year(x.year)))\n",
    "\n",
    "# Step 5. Set the right dates as the index\n",
    "data.index = pd.to_datetime(data.index)\n",
    "\n",
    "# Step 6. Compute how many values are missing for each location over the entire record\n",
    "missing_values_per_location = data.isnull().sum()\n",
    "print(\"Missing values for each location:\")\n",
    "print(missing_values_per_location)\n",
    "\n",
    "# Step 7. Compute how many non-missing values there are in total\n",
    "total_non_missing_values = data.notnull().sum().sum()\n",
    "print(\"\\nTotal non-missing values:\", total_non_missing_values)\n",
    "\n",
    "# Step 8. Calculate the mean windspeeds of the windspeeds over all the locations and all the times\n",
    "mean_windspeed = data.mean().mean()\n",
    "print(\"\\nMean windspeed:\", mean_windspeed)\n",
    "\n",
    "# Step 9. Create a DataFrame called loc_stats and calculate the min, max, mean, and standard deviations of the windspeeds at each location\n",
    "loc_stats = pd.DataFrame()\n",
    "loc_stats['Min'] = data.min()\n",
    "loc_stats['Max'] = data.max()\n",
    "loc_stats['Mean'] = data.mean()\n",
    "loc_stats['Std'] = data.std()\n",
    "print(\"\\nLocation statistics:\")\n",
    "print(loc_stats)\n",
    "\n",
    "# Step 10. Create a DataFrame called day_stats and calculate the min, max, mean, and standard deviations of the windspeeds across all the locations at each day\n",
    "day_stats = pd.DataFrame()\n",
    "day_stats['Min'] = data.min(axis=1)\n",
    "day_stats['Max'] = data.max(axis=1)\n",
    "day_stats['Mean'] = data.mean(axis=1)\n",
    "day_stats['Std'] = data.std(axis=1)\n",
    "print(\"\\nDay statistics:\")\n",
    "print(day_stats)\n",
    "\n",
    "# Step 11. Find the average windspeed in January for each location\n",
    "january_mean_windspeeds = data[data.index.month == 1].mean()\n",
    "print(\"\\nAverage windspeed in January for each location:\")\n",
    "print(january_mean_windspeeds)\n",
    "\n",
    "# Step 12. Downsample the record to a yearly frequency for each location\n",
    "yearly_data = data.resample('Y').mean()\n",
    "print(\"\\nYearly frequency for each location:\")\n",
    "print(yearly_data)\n",
    "\n",
    "# Step 13. Downsample the record to a monthly frequency for each location\n",
    "monthly_data = data.resample('M').mean()\n",
    "print(\"\\nMonthly frequency for each location:\")\n",
    "print(monthly_data)\n",
    "\n",
    "# Step 14. Downsample the record to a weekly frequency for each location\n",
    "weekly_data = data.resample('W').mean()\n",
    "print(\"\\nWeekly frequency for each location:\")\n",
    "print(weekly_data)\n",
    "\n",
    "# Step 15. Calculate the min, max, mean, and standard deviations of the windspeeds across all locations for each week for the first 52 weeks\n",
    "weekly_stats = data.resample('W').agg(['min', 'max', 'mean', 'std'])\n",
    "print(\"\\nWeekly statistics for the first 52 weeks:\")\n",
    "print(weekly_stats.iloc[:52])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93428ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
